\documentclass[anon,12pt]{colt2016} % Anonymized submission


%\documentclass{colt2016} % Include author names
% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\def\COMPLETE{}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{nickstyle_ICML}

\usepackage{hyperref}
\usepackage[toc,page]{appendix}

\usepackage{color}
\usepackage[toc,page]{appendix}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{times}

\usepackage{float}
\usepackage{caption2}

\usepackage{tikz}
\usetikzlibrary{shapes, calc, arrows, through, intersections, decorations.pathreplacing, patterns}

\newtheorem{fact}[theorem]{Fact}
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{claim}[theorem]{Claim}

% Simple environments for algorithms
\newenvironment{alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
    }
}
{
    \end{list}
}

%\floatstyle{ruled}
 %\newfloat{algorithm}{ht}{lop}
\floatname{algorithm}{Algorithm}
\makeatletter
\renewcommand{\floatc@ruled}[2]{\vspace{3pt}{\@fs@cfont #1:\:} #2 \par \vspace{2pt}}
\makeatother


\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbf}
\newcommand{\todo}{\textcolor{blue}{[TODO]}\xspace}
\newcommand{\fix}{\textcolor{blue}{[FIX]}\xspace}
%\DeclareMathOperator{\argmin}{argmin}


 \coltauthor{\Name{Shai Ben-David} \Email{shai@cs.uwaterloo.ca }\\
 \addr University of Waterloo
 \AND
 \Name{Shrinu Kushagra} \Email{skushagr@uwaterloo.ca }\\
 \addr University of Waterloo
 \AND
 \Name{Samira Samadi} \Email{ssamadi6@gatech.edu}\\
 \addr Georgia Institute of Technology
 }


\title[Short Title]{Clustering under structural noise}
\begin{document}
\maketitle

\begin{abstract}
\todo
\end{abstract}

\begin{keywords}
Clustering, $\alpha$-center proximity, $\lambda$-center separation
\end{keywords}

\section{Introduction}
\subsection{Motivation}
\todo Talk about CDNM Thesis. Clustering is difficult only when it doesn't matter. Niceness assumptions on data. If data has these properties clustering is easy.

\subsection{Related Work}
\todo 2 such assumptions. $\lambda$-center separation (\cite{ben2014clustering}) and $\alpha$-center proximity (\cite{awasthi2012center} and \cite{balcan2012clustering}). If the optimal solution to the center-based objective satisfies these nice properties then they have poly. time algorithms to find that optimal solution. They consider relaxation of this assumption in the presence of noise, i.e., when the data is allowed to have atmost $\epsilon$ fraction of the points which do not satisfy the niceness properties.

\subsection{Our approach}
\todo Our approach is different in two regards.
\begin{itemize}[nolistsep]
\item We dont work with a particular cost function. Give poly. time alg.  that work for any clustering that has the nice property.
\item No size restriction on the noise. Assume that it doesn't have a structure. 
\end{itemize}
We give poly. alg. which capture all nice clustering of a dataset as long as the noise is unstructured (or sparse).

\subsection{Comparison with previous work}
\todo For $(\alpha, \epsilon)$-center proximity. \cite{balcan2012clustering} obtain an $1+\epsilon$ to the cost of the optimal $k$-median solution if $\alpha \ge 2+\sqrt{7}$ and atmost $\epsilon$-fraction of the dataset is noise. Their algorithm is also able to capture all $(\alpha, \epsilon)$-nice clusterings. We show that if $\alpha \le 2\sqrt{2} + 3$ and the number of noisy points is large, then no algorithm can output a tree which captures all nice clusterings. We can overcome this lower bound if we assume that the noise is sparse. We give an alg. which captures all $(\alpha, \eta)$-clusterings if the noise is $\eta$-sparse and $\alpha \ge 2+\sqrt{7}$.

For $\lambda$-center separation. Prove that problem is NP-Hard for $\lambda \le 2$. For $\lambda \ge 3$, give an algorithm which outputs the (unique) clustering.

For $(\lambda, \epsilon)$-center separation \cite{ben2014clustering} give a generic algorithm for any center-based objective function. The separation parameter $\lambda$ required is very large. For $k$-median, the required separation can be as big as $16$. We show that if $\lambda \le 6$ and the number of noisy points is large, then no algorithm can capture all nice clusterings. We can overcome this lower bound if we assume that the noise is sparse. We give poly. alg for $\lambda \ge 4$ for sparse noise.


\subsection{Structure of paper}
\todo Following is the proposed structure of the paper.
\vspace{-0.1in}\begin{enumerate}
\setlength\itemsep{0em}
\item Introduction
\item Notations and definition
\item $\alpha$-center proximity	\vspace{-0.1in}
	\begin{enumerate}[leftmargin=32pt]
	\renewcommand{\labelenumii}{\theenumii}
	\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
	\setlength\itemsep{0em}
	\item Lower Bound ($\alpha \le 2\sqrt{2} + 3$) under arbitrary noise.
	\item Positive result for $\alpha \ge 2+\sqrt{7}$.
	\item Lower bound ($\alpha \le 2+\sqrt{3}$) under sparse noise.
	\end{enumerate}
\item $\lambda$-center separation \vspace{-0.1in}
	\begin{enumerate}[leftmargin=32pt]
	\renewcommand{\labelenumii}{\theenumii}
	\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
	\setlength\itemsep{0em}
	\item Positive result for $\lambda \ge 3$ with no noise.
	\item NP-Hard for $\lambda \le 2$ with no noise.
	\item Lower Bound ($\lambda \le 6$) under arbitrary noise.
	\item Positive result for $\lambda \ge 4$ under sparse noise.
	\item Lower bound ($\lambda < 4$) under sparse noise.
	\end{enumerate}
\item Conclusion
\end{enumerate}


\section{Notation and definition}
We are given a data set $\mc X$ drawn from some metric space $(\mb M, d)$ and an integer $k$. A $k$-clustering of $\mc X$ denoted by $\mc C_{\mc X}$ is a partition of $\mc X$ into $k$ disjoints sets $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$. Given points $c_1, \ldots, c_k \in \mb M$, we can define the clustering induced by these points (or {\it centers}) such that each $x \in \mc X$ is assigned to its nearest center. Given $\mc S \subseteq \mc X$, we define $\mc C_{\mc X}|_{\mc S}$ as the clustering $\mc C_{\mc X}$ restricted to the set $\mc S$. Formally, $\mc C_{\mc X}|_{\mc S} = \{C_1 \cap S, \ldots, C_k \cap S\}$.

We will use the notation $c(\mc A)$ to denote the center of a set $\mc A$. For any set $\mc A\subseteq \mc X$ with centre $c$, we define the radius of $\mc A$ as $r(\mc A) = \max_{x \in \mc A} d(x, c)$. We denote a ball of radius $r$ at centre $x$ by $B(x, r)$.

Given $\mc S \subseteq \mc X$. Let $\mc C_{\mc S} = \{S_1, \ldots, S_{k'}\}$ be a clustering of $\mc S$. We say that $\mc C_{\mc X}$ respects $\mc C_{\mc S}$ if for all $S_i \in \mc C_{\mc S}$ there exists $C_j \in \mc C_{\mc X}$ such that $C_i \subseteq C_j$ and for all $m \neq i, S_m \cap C_j = \phi$.

%\begin{definition}[$\mc C_{\mc X}$ respects $\mc C_{\mc S}$] Given a clustering instance $(\mc X, d)$ and $\mc S \subseteq \mc X$. Let $\mc C_{\mc X} = \{ C_1, \ldots, C_{k'} \}$ be a clustering of $\mc X$ and $\mc C_{\mc S} = \{S_1, \ldots, S_k\}$ be a clustering of $\mc S$. We say that $\mc C_{\mc X}$ respects $\mc C_{\mc S}$ if for all $S_i \in \mc C_{\mc S}$ there exists $C_j \in \mc C_{\mc X}$ such that $C_i \subseteq C_j$ and for all $m \neq i, S_m \cap C_j = \phi$.
%\label{defn:cxrespectscs}
%\end{definition}

%\begin{definition}[$\mc C_{\mc X}$ restricted to $\mc S$] Given a clustering instance $(\mc X, d)$ and $\mc S \subseteq \mc X$. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be a clustering of $\mc X$. We define $\mc C_{\mc X}|_{\mc S}$ as the clustering $\mc C_{\mc X}$ restricted to the set $\mc S$. Formally, $\mc C_{\mc X}|_{\mc S} = \{C_1 \cap S, \ldots, C_k \cap S\}$. Note that $\mc C_{\mc X}|_{\mc S}$ is a clustering of the set $\mc S$. 
%\end{definition}

\begin{definition}[$\alpha$-center proximity]
\label{defn:alphacp}
Given a clustering instance $\mc X$ drawn from some metric space $(\mb M, d)$ and the number of clusters $k$. We say that a clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $c_1, \ldots, c_k \in \mb M$ has $\alpha$-center proximity w.r.t $\mc X$ and $k$ if the following holds. For all $x \in C_i$ and $i\neq j$, 
$$\alpha d(x, c_i) < d(x, c_j)$$
\end{definition}

\noindent The set $\mc X$ is {\it $\alpha$-center} {\it $t$-min proximal} if there exist a clustering $\mc C_{\mc X}=\{C_1,\ldots,C_k\}$ which satisfies $\alpha$-center proximity and $\min\limits_{i} \lvert C_i\rvert = t$. 

\todo Motivate $\eta$-sparse definition.

\begin{definition}[$(\alpha, \eta)$-center proximity]
Given a clustering instance $\mc X$ drawn from some metric space $(\mb M, d)$, the number of clusters $k$ and $\mc S \subseteq \mc X$. We say that a clustering $\mc C_{\mc S} = \{S_1, \ldots, S_k\}$ induced by centers $s_1, \ldots, s_k \in \mb M$ has $(\alpha, \eta)$-center proximity w.r.t $\mc X, \mc S$ and $k$ if the following holds.

\begin{itemize}[nolistsep, noitemsep]
\label{defn:alphacpnoise}	

\item[$\diamond$] {\bf $\alpha$-centre proximity}: For all $x \in S_i$ and $i\neq j$, $\thinspace\alpha d(x, s_i) < d(x, s_j)$
\item[$\diamond$]{\bf $\eta$-sparse noise}: For any ball $B$ such that $c(B)\in \mathcal{X}$, if $r(B)\leq \eta\max\limits_{i} \thinspace r(S_i)$, then $|B\cap (\mc X\setminus \mc S)| < \min\limits_{i} |S_i|/2$
\end{itemize}
\end{definition}

\noindent A set $\mc S \subseteq \mc X$ is {\it $(\alpha, \eta)$-center} {\it $t$-min $r$-max proximal} if there exist a clustering $\mc C_{\mc S}=\{S_1,\ldots,S_k\}$ which satisfies $(\alpha, \eta)$-center proximity and $\min\limits_{i} \lvert S_i\rvert = t$ and $\max\limits_{i} r(S_i) = r$. Note that given $(\mc X, d), k$, $t$ and $r$ there can be several such $\mc S$ and several clusterings $\mc C_{\mc S}$ which satisfy these conditions.

\todo Add example.


\section{Center Proximity}

%\noindent A clustering instance $(\mc X, d)$ satisfies $(\alpha, \eta)$-center proximity if there exist $\mc S \subseteq \mc X$ such that there exists a clustering $\mc C$ of $\mc S$ which has $(\alpha, \eta)$-center proximity w.r.t $\mc X, \mc S$ and $k$.

The problem of clustering under the presence of noise was first considered by \cite{balcan2012clustering}. They assumed that the  

\subsection{Goal}
We are given a clustering instance $(\mc X, d)$, the number of clusters $k$ and a parameter $t$. Our goal is to output a hierarchical clustering tree of $\mc X$ which has the following property. For every $\mc S \subseteq \mc X$ which is $(\alpha, \eta)$-center $t$-min nice, and every clustering $\mc C_{\mc S}$ which satisfies $(\alpha, \eta)$-center proximity, there exists a pruning $\mc P$ of the tree such that $\mc C_{\mc X}^{\mc P}$ (the clustering of the set $\mc X$ corresponding to the pruning $\mc P$) respects $\mc C_{\mc S}$. 

In the next section, we introduce a hierarchical clustering algorithm (Alg.\ref{alg:alphacp}) and prove (Thm.\ref{thm:alphacpnoise}) that our algorithm indeed achieves the above mentioned goal (under certain assumptions on the parameters $\alpha$ and $\eta$).
It is important to note that our algorithm only knows $\mc X$ and has no knowledge of the set $\mc S$.
% which is based on a novel distance function.
The output of our algorithm is a clustering tree that is able to capture all $(\alpha, \eta)$-center $t$-min nice subsets $\mc S$. More precisely, this tree has a pruning $\mc P$ such that the corresponding clustering $\mc C_{\mc X}^{\mc P}$ respects the clustering $\mc C_{\mc S}$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Algorithm}
Balcan and Liang studied the problem of clustering in the presence of noise under similar assumptions. They consider a data set $\mc X$ such that the optimal clustering of $\mc X$ w.r.t some objective function has $\alpha$-center proximity (Defn.\ref{defn:alphacp}) except for an $\epsilon$ fraction of the points. They presented a polynomial-time algorithm that gives a $(1+O(\epsilon))$-approximation to the cost of optimal clustering.

In this work, we consider a similar problem. However, our approach is different in two aspects. Firstly, we do not place any restriction on the size of the noise but assume that it is {\it sparse} and doesn't have any {\it structure}. Secondly, we don't work with any particular objective function. Our goal is to ``capture" all clusterings which have $(\alpha, \eta)$-center proximity.

Our algorithm uses a linkage-based procedure based on a novel distance condition. Before describing our algorithm, we need to define our {\it Sparse distance condition}.

\begin{definition}[Sparse distance condition]
	 Given a clustering $\mc C_{\mc X}=\{C_1,\ldots,C_k\}$ and $p, q \in \mc X$. Let $B = B(p, d(p, q))$. Define $Y_B^{\mc C_{\mc X}} := \{C_i \in \mc C_{\mc X} : C_i \subseteq B \text{ or } |B \cap C_i| \ge t/2\}$. 
We say that the ball $B$ satisfies the sparse distance condition w.r.t clustering $\mc C$ when the following holds.
\begin{itemize}[noitemsep, leftmargin=*]
\item $|B| \ge t$.
\item For any $C_i \in \mc C_{\mc X}$, if $C_i \cap B \neq \phi$, then $C_i \in Y_B^{\mc C}$.
\end{itemize}
\end{definition}

Intuitively, Alg.\ref{alg:alphacp} works as follows. It maintains a clustering $\mc C_{\mc X}$, which is initialized so that  each point is in its own cluster. It then iterates over all pairs of points $p, q$ in increasing order of their distance $d(p, q)$. If $B(p, d(p,q))$ satisfies the sparse distance condition w.r.t $\mc C_{\mc X}$, then it merges all the clusters which intersect with this ball into a single cluster and updates $\mc C_{\mc X}$. Furthermore, the algrorithm builds a tree with the nodes corresponding to the merges performed so far. We will show that for all $\mc S \subseteq \mc X$ and for all clusterings $\mc C_S$ which have $(\alpha, \eta)$-center proximity and $\min\limits_{C_i \in {\mc C}_{\mc S}} |C_i| = t$, Alg. \ref{alg:alphacp} outputs a tree such that there exists a pruning $\mc P$ such that the clustering $\mc C_X$ corresponding to the pruning respects $\mc C_S$. %which {\color{red}[}respects the clustering $\mc C$ {\color{red}] What does "respect" mean here?}.

	
\begin{algorithm}
%\begin{alg}
	\SetAlgoLined
	\KwIn{$(\mc X, d), k$ and $t$}
	\KwOut{A hierarchical clustering tree $T$ of $\mc X$.}
	
	Let $\mc C^{(l)}$ denote the clustering $\mc X$ after $l$ merge steps have been performed. Initialize $\mc C^{(0)}$ so that all points are in their own cluster. That is, $\mc C^{(0)} = \{ \{x\}: x \in \mc X\}$.
	
	Iterate over all pairs of points $p, q$ in increasing order of the distance $d(p, q)$. If $B = B(p, d(p, q))$ satisfies the sparse distance condition then
	\begin{itemize}
	\renewcommand\labelitemi{}
		\item $C_{temp} = \phi$ and $\mc C^{(i)} = \mc C^{(i-1)}$
		\item for all $C \in Y_B^{C^{(i-1)}}$.
		\begin{itemize}
		\renewcommand\labelitemii{}
			\item $\mc C^{(i)} = \mc C^{(i)} \setminus C$ and $C_{temp} = C_{temp} \cup C$
		\end{itemize}
		\item $\mc C^{(i)} = \mc C^{(i)} \cup C_{temp}$.
		\item This step basically merges all the clusters in $Y_B^{C^{(i-1)}}$ into a single cluster.
	\end{itemize}
	
	
	Output clustering tree $T$. The leaves of $T$ are the points in dataset $\mc X$. The internal nodes corresppond to the merging performed in the previous step.
	%\item[4] Construct $T$ from $T'$ by deleting the all nodes which do not have any children.
%\end{alg}
\caption{Alg. for $(\alpha, \eta)$-center proximity with parameter $t$}
\label{alg:alphacp}
\end{algorithm}


\begin{theorem}
\label{thm:alphacpnoise}
Given a clustering instance $(\mc X, d)$, the number of clusters $k$ and a parameter $t$. Alg. \ref{alg:alphacp} outputs a tree $T$ with the following property. For all $\mc S \subseteq \mc X$ and for all clusterings $\mc C^*_{\mc S} = \{S_1^*, \ldots, S_k^*\}$ induced by centers $s_1, \ldots, s_k \in \mc X$ which satisfy $(\alpha, \eta)$-center proximity w.r.t $\mc X, \mc S$ and $k$ and $ \min_i|S_i^*| = t \ge 2$, the following holds. 

If $\alpha \ge 2 + \sqrt 7$ and $\eta \ge 1$, then for every $1\le i \le k$, there exists a node $N_i$ in the tree $T$ such that $S_i^* \subseteq N_i$ and for $j \neq i$, $S_j^* \cap N_i = \phi$ . That is, $N_i$ contains points from only one of the good clusters $S_i^*$.
%{\color{red} Is it obvious that the nodes in the pruning are disjoint and they form a clustering of set X?} 
\end{theorem}

\begin{proof}
Fix any $\mc S \subseteq \mc X$. Let $\mc C^*_S = \{S_1^*, \ldots, S_k^*\}$ be a clustering of $\mc S$ such that $\min |S_i^*| = t$ and $\mc C^*_S$ has $(\alpha, \eta)$-center proximity. Denote by $r_i := r(S_i^*)$ and define $r := \max r_i$. Let $\mc C^{(l)} = \{C_1, \ldots, C_{k'}\}$ be the clustering of $\mc X$ after $l$ merge steps have been performed (as defined in Alg.\ref{alg:alphacp}). Let $p, q \in \mc X$ be the pair of points being considered at the $(l+1)^{th}$ step. Let's denote $B = B(p, d(p, q))$. Note that whenever $B$ satisfies the sparse-distance condition, all the clusters in $Y_{B}^{{\mc C}^{(l)}}$ are merged together and the clustering $\mc C^{(l+1)}$ is updated. Throughout the proof, we will denote by $S_i^* \in \mc C^*_S$ the clusters of the set $\mc S$ (also called ``good" clusters) and by $C_i \in \mc C^{(l)}$ the clusters of the set $\mc X$ obtained after $l$ merge operations.

\noindent We will prove the theorem by proving two key facts.

\begin{enumerate}[nolistsep, noitemsep, label=\textbf{F.\arabic*}]
\renewcommand\labelitemi{$\diamond$}
\item \label{fact:1} If the algorithm merges points from two good clusters, then at this step the distance being considered $d = d(p,q) > r_i$.	
\item \label{fact:2} When the algorithm considers the distance $d = d(s_i, q_i) = r_i$ (the radius of the $i^{th}$ good cluster $S_i^*$), it merges all points from $S_i^*$ (and possibly points from $\mc X\setminus \mc S$) into a single cluster $C_i$. Hence, there exists a node in the tree $N_i$ which contains all the points from $S_i^*$ and no points from any other good cluster $S_j^*$. 	
\end{enumerate}
Note that the theorem follows from these two facts. We now state both of these facts formally below and prove them.

\noindent\textit{\underline{Proof of Fact \ref{fact:1} %{\color{blue} [make reference to the corresponding fact]}
}}\\
Let $\mc C^{(l)} = \{C_1, \ldots, C_{k'}\}$ be the current clustering of $\mc X$. Let $l+1$ be the first merge step which merges points from the good cluster $S_i^*$ with points from some other good cluster. Let $p, q \in \mc X$ be the pair of points being considered at this step and $B = B(p, d(p, q))$ the ball that satisfies the sparse distance condition at this merge step. Denote by $Y = Y_{B}^{C^{(l)}}$.

We need to show that $d(p, q) > r_i$. However, before we can prove that we need another result which is proved as a claim below. Claim \ref{claim:fromBothCluster} basically shows that the ball $B$ contains points from two good clusters $S_i^*$ and $S_n^*$. We then prove the desired result (Claim \ref{claim:maxrirj}).
\begin{claim}
\label{claim:fromBothCluster}
Let $p, q \in \mc X$ and $B$, $Y$, $S_i^*$ and $C^{(l)}$ be as defined above. If $d(p, q) \le r,$ then $B \cap S_i^* \neq \phi$ and there exists $n \neq i$ such that $B \cap S_n^* \neq \phi$.
\end{claim}
\vspace{-0.1in} Observe that $l+1$ is the first step which merges points from $S_i^*$ with some other good cluster. Hence, there exists a cluster $C_i \in Y$ such that $C_i\cap S_i^*  \neq \phi$ and for all $n \neq i$, $C_i \cap S_n^* = \phi$. Also, there exists cluster $C_j \in Y$ such that $C_j \cap S_j^* \neq \phi$ for some $S_j^*$ and $C_j \cap S_i^* = \phi$.

$C_i \in Y$. Hence, $C_i \subseteq B$ or $|C_i \cap B| \ge t/2$. In the first case, if $C_i \subseteq B$ then $B \cap S_i* \neq \phi$ by set inclusion. In the second case assume that $|C_i \cap B| \ge t/2$. For the sake of contradiction, assume that $B$ contains no points from $S_i^*$. That is, $B \cap C_i \subseteq C_i \setminus S_i^* \subseteq \mc X \setminus \mc S$. This implies that $B \cap C_i \subseteq B \cap \{\mc X \setminus \mc S\}$. Hence, $|B\cap \{\mc X \setminus \mc S\}| \ge |B \cap C_i| \ge t/2$. This contradicts the sparse noise assumption in Defn. \ref{defn:alphacpnoise}. Hence, $B \cap S_i^* \neq \phi$.

$C_j \in Y$. Hence, $C_j \subseteq B$ or $|C_j \cap B| \ge t/2$. In the first case, if $C_j \subseteq B$ then $B \cap S_j^* \neq \phi$ by set inclusion. In the second case assume that $|C_j \cap B| \ge t/2$. For the sake of contradiction, assume that for all $n \neq i$, $B$ contains no points from $S_n^*$. That is, $B \cap C_j \subseteq C_j \setminus (\cup_{n \neq i} S_n^*) \subseteq \mc X \setminus \mc S$. This implies that $B \cap C_j \subseteq B \cap \{\mc X \setminus \mc S\}$. Hence, $|B\cap \{\mc X \setminus \mc S\}| \ge |B \cap C_j| \ge t/2$. This contradicts the sparse noise assumption in Defn. \ref{defn:alphacpnoise}. Hence, there exists $S_n^* \neq S_i^*$ such that $B \cap S_n^* \neq \phi$.

\begin{claim}
\label{claim:maxrirj}
Let the framework be as given in Claim \ref{claim:fromBothCluster}. Then, $d(p, q) > r_i$.
\end{claim}

\vspace{-0.1in} If $d(p, q) > r$, then the claim follows trivially. We assume that $d(p, q) \le r$. From claim \ref{claim:fromBothCluster}, $B$ contains $p_i \in S_i^*$ and $p_j \in S_j^*$. Let $r_i = d(c_i, q_i)$ for some $q_i \in S_i^*$.
\begin{align*}
d(c_i, q_i) &< \frac{1}{\alpha} d(q_i, c_j) \le \frac{1}{\alpha} \bigg[ d(p_j, c_j) + d(p_i, p_j) + d(p_i, q_i)\bigg] < \frac{1}{\alpha} \bigg[ \frac{1}{\alpha}d(p_j, c_i) + d(p_i, p_j) + 2d(c_i, q_i)\bigg]\\
& < \frac{1}{\alpha} \bigg[ \frac{1}{\alpha}d(p_i, p_j) + \frac{1}{\alpha}d(c_i, q_i) + d(p_i, p_j) + 2d(c_i, q_i)\bigg]
\end{align*}
This implies that $(\alpha^2 - 2\alpha - 1)d(q_i, c_i) < (\alpha + 1) d(p_i, p_j)$. For $\alpha \ge 2 + \sqrt 7$, this implies that $d(c_i, q_i) < d(p_i, p_j)/2$. Now, using triangle inequality, we get that $d(c_i, q_i) < d(p_i, p_j)/2 \le \frac{1}{2}[d(p, p_i) + d(p, p_j)] < d(p, q)$.\\

\noindent\textit{\underline{Proof of Fact \ref{fact:2}
}}\\
Let $\mc C^{(l)} = \{C_1, \ldots, C_{k'}\}$ be the current clustering of $\mc X$. Let $l+1$ be the merge step when $p = s_i$ and $q = q_i$ such that $d(s_i, q_i) = r_i$. We will prove that the ball $B = B(s_i, q_i)$ satisfies the sparse-distance condition. Hence, all the points from the good cluster $S_i^*$ will be merged together. Note that this step merges all the clusters in $Y = Y_B^{C^{(l)}}$. Hence to complete the proof, we also show that all the clusters in $Y$ don't intersect with any other good cluster $S_j^*$. We state this formally below and then prove it.

\begin{claim}
%\vspace{-0.1in}
\label{claim:dciqi}
Let $s_i$, $q_i$, $r_i$, $B$ and $Y$ be as defined above. Then, $B$ satisfies the sparse distance condition and for all $C \in Y$, for all $j \neq i, C \cap S_j^* = \phi$.
\end{claim}

\vspace{-0.1in} Denote by $B = B(s_i, q_i)$. $|B| = |S_i^*| \ge t$. Observe that, for all $C \in \mc C^{(l)}$, $|C| = 1$ or $|C| \ge t$. We need to prove two statements. If $C \cap B \neq \phi$, then $C \in Y$ and $C \cap S_j^* = \phi$. 

\begin{itemize}[nolistsep]
\item Case 1. $|C| = 1$. If $C \cap B \neq \phi \implies C \subseteq B = S_i^*$. Hence, $C \in Y$ and for all $j \neq i$, $C \cap S_j^* = \phi$

\item Case 2. $|C|\ge t$. $C \cap B \neq \phi$. Let $h(C)$ denote the height of the cluster in the tree $T$. Now, we consider two subcases.
\begin{itemize}
\renewcommand\labelitemii{$\circ$}
\item Case 2.1. $h(C) = 1$. In this case, there exists a ball $B'$ such that $B' = C$. We know that $r(B') \le r_i \le r$. Hence using Claim \ref{claim:maxrirj}, we get that for all $j \neq i$, $B' \cap S_j^* = \phi$. Thus, $|B'\setminus S_i^*| \le t/2 \implies |B\cap C| = |C| - |C\setminus B| = |C| - |B'\setminus S_i^*| \ge t/2$. Hence, $C \in Y$.

\item Case 2.2. $h(C) > 1$. Then there exists some $C'$ such that $h(C') = 1$ and $C' \subset C$. Now, using set inclusion and the result from the first case, we get that $|B\cap C| \ge |B\cap C'| \ge t/2$. Hence, $C \in Y$. Using Claim \ref{claim:maxrirj}, we get that for all $j \neq i$, $C \cap S_j^* = \phi$.
\end{itemize} 
\end{itemize}
\end{proof}

\begin{theorem}
Given clustering instance $(\mc X, d)$, $k$ and $t$. Algorithm \ref{alg:alphacp} runs in time which is polynomial in $|\mc X|$.
\end{theorem}

\begin{proof}
Let $n = |\mc X|$. Let $\mc C^{(l)} =\{C_1, \ldots, C_{k'}\}$ denote the current clustering of $\mc X$ as defined in Alg. \ref{alg:alphacp}. Observe that given the ball $B = B(p, d(p, q))$, for any cluster $C_i \in \mc C^{(l)}$, checking if $C_i \in Y_B^{C^{(l)}}$ takes time O($|C_i|$). Doing this for all clusters takes $O(n)$ time. The algorithm examines all the pairs of points and hence runs in $O(n^3)$ time.
\end{proof}

\subsection{Lower Bounds}
\label{sec:alphaLBd}
Recall the framework in the previous sections. We were given a clustering instance $(\mc X, d)$, the number of clusters $k$ and a parameter $t$. Our goal was to output a hierarchical clustering tree of $\mc X$ which has the following property. ``{\it For every $\mc S \subseteq \mc X$ which is $(\alpha, \eta)$-center $t$-min nice, and every clustering $\mc C_{\mc S}$ which satisfies $(\alpha, \eta)$-center proximity, there exists a pruning $\mc P$ of the tree such that the clustering corresponding to the pruning $\mc C_{\mc X}^{\mc P}$ respects $\mc C_{\mc S}$.}'' Whenever, a tree has this property, we will say that the tree {\it captures} all $(\alpha, \eta)$-center $t$-min nice subsets of $\mc X$.

We proposed a polynomial-time algorithm (Alg. \ref{alg:alphacp}) and proved that for $\alpha \ge 2+\sqrt 7$ and $\eta \ge 1$, our algorithm achieves the desired objective. Another natural question to ask is that ``under what conditions or values of $\alpha$ and $\eta$ is it not possible to achieve our goal?'' The general flavour of results would be the following. We will show that if the value of $\alpha$ is below a certain threshold say $\alpha_0$ then no algorithm can succeed in achieving the desired goal. We first describe our lower bound results informally.

\begin{enumerate}[nolistsep, noitemsep, label=\textbf{L.\arabic*}]
\renewcommand\labelitemi{$\diamond$}
\item \label{lowerBd:alphacp1} If $\alpha < 2 + \sqrt{5}$ and $\eta \le 1$ (noise is sparse), then the tree outputted by Alg. \ref{alg:alphacp} doesn't capture all $(\alpha, \eta)$-center $t$-min nice subsets of $\mc X$. 
\item \label{lowerBd:alphacp2} If $\alpha < 2 + \sqrt{3}$ and $\eta \le 1$ (noise is sparse), then no algorithm can output a tree such that the tree captures all $(\alpha, \eta)$-center $t$-min nice subsets of $\mc X$.
\item \label{lowerBd:alphacp3} If $\alpha \le 2\sqrt{2}+3$ and the number of noisy points $ > 3t/2+5$, then no algorithm can output a tree such that the tree captures all $\alpha$-center $t$-min nice subsets of $\mc X$. Note that in this case, we do not assume that the noise is sparse.
\end{enumerate}

\noindent Its important to note that \ref{lowerBd:alphacp1} and \ref{lowerBd:alphacp2} are lower bounds for the case when we assume that the noise is sparse. However, \ref{lowerBd:alphacp3} shows that if we do not have assume that the noise is sparse then even for a large value of $\alpha$ no algorithm can `tolerate' $3t/2+5$ noisy points where $t$ is the size of the minimum cluster. Note that Alg. \ref{alg:alphacp} can `tolerate' much more than $3t/2+5$ noisy points as long the noise is {\it unstructured} or sparse. We will now prove each of the lower bound results.

The proof of the lower bounds will follow the follwing general structure. Firstly, we will construct the set $\mc X$, then we will construct sets $\mc S \subset \mc X$ which are $(\alpha, \eta)$-center $t$-min nice (for \ref{lowerBd:alphacp1} and \ref{lowerBd:alphacp2}) or $\alpha$-center $t$-min nice (for \ref{lowerBd:alphacp3}). Then we show that all the clusterings can not be captured by the output tree. Before, we begin the proof of the lower bounds, we need to introduce some notation. 

Given the number of clusters $k$ and parameter $t$. Denote by $P_{i}(c) := B(c, 0)$ a ball of radius $0$ centered at $c$ such that $|B(c, 0)| = i$. We will sometimes omit $c$ and use the notation $P_i$ when the center is clear from the context. Recall that we defined what it means for a clustering $\mc C_{\mc X}$ to respect $\mc C_{\mc S}$ (Defn. \ref{defn:cxrespectscs}). With a slight abuse of notation, we will say that a pruning $P$ of a hierarchical clustering tree $T$ of $\mc X$ respects $\mc C_{\mc S}$, if the clustering corresponding to the pruning $\mc C_{\mc X}^P$ respects $\mc C_{\mc S}$. Also, if such a pruning $P$ exists, we will say that the tree $T$ respects $\mc C_{\mc S}$.

\begin{figure}[!ht]
\input{../figures/lbdFig1}
\caption{$\mc X \subseteq \mathbb{R}$ such that Alg. \ref{alg:alphacp} doesn't capture all the $(\alpha, \eta)$-center $t$-min nice subsets of $\mc X$}
\label{fig:algAlphacp}
\end{figure}

\begin{theorem}
\label{thm:algAlphacp}
Given the number of clusters $k$ and parameters $\alpha$, $\eta$ and $t$. There exists a clustering instance $(\mc X, d)$ such that Alg. \ref{alg:alphacp} outputs a tree $T$ with the following property. There exists $\mc S \subseteq \mc X$ and there exists clustering $\mc C_{\mc S} = \{S_1, \ldots, S_k\}$ induced by centers $s_1, \ldots, s_k \in \mc X$ which satisfies $(\alpha, \eta)$-center proximity w.r.t $\mc X, \mc S$ and $k$ and $ \min_i|S_i| = t \ge 2$, such that the following holds. 

If $\alpha < 2 + \sqrt 5$ and $\eta \le 1$ and then there doesn't exist a pruning of the tree such that the corresponding clustering $\mc C_{\mc X}$ respects $\mc C_{\mc S}$.
\end{theorem}

\begin{proof}
Define $\mc X \subseteq \mathbb{R}$ as follows. 

Let $t' = \frac{t}{2}-1$. Let $O_1 = \{P_{t'}(0) \thinspace\cup\thinspace P_{t'}(2\alpha) \thinspace\cup\thinspace P_{t'}(4\alpha+2) \thinspace\cup\thinspace P_{t'}(6\alpha+2)\}$ and $O_2 = \{P_{2}(\alpha+1) \thinspace\cup\thinspace P_{2}(3\alpha+1) \thinspace\cup\thinspace P_{2}(5\alpha+1) \}$. For $3\le i\le k$, define $B_i = P_t (16\alpha+2+i \alpha )$. Now, let $\mc X = \{ O_1 \cup O_2 \cup B_3 \cup \ldots \cup B_k \}$. The set $\mc X$ is also shown in Fig. \ref{fig:algAlphacp}.

Now, let $B_1 = \{P_{t'}(0) \thinspace\cup\thinspace  P_{2}(\alpha+1) \thinspace\cup\thinspace P_{t'}(2\alpha) \}$ and $B_2 = \{\ P_{t'}(4\alpha+2) \thinspace\cup\thinspace P_{2}(5\alpha+1)\thinspace\cup\thinspace P_{t'}(6\alpha+2)\}$. Now, define $\mc S = \{B_1 \cup \ldots \cup B_k\} \subseteq \mc X$. It is easy to see that for $\alpha \le 2+\sqrt{5}$, $\mc S$ is $(\alpha, 1)$-center $t$-min nice and the clustering $\mc C_{\mc S} = \{B_1, \ldots, B_k\}$ satisfies $(\alpha, 1)$-center proximity. However, we will show that the tree outputted by Alg. \ref{alg:alphacp} doesn't contain a pruning such that the corresponding clustering respects $\mc C_{\mc S}$.

Denote by $B_{12} = \{P_{t'}(2\alpha) \cup P_{2}(3\alpha+1) \cup P_{t'}(4\alpha+2) \}$. Alg. \ref{alg:alphacp}, iterates through pairs of points in increasing order of their distance. Hence, it first merges points in $B_3$ to $B_k$ into $k-2$ clusters. Now, observe that $B_1, B_{12}$ and $B_{2}$ have the radius $\alpha + 1$ (as our centers must be part of the dataset $\mc X$). Hence, the algorithm can choose to merge points in $B_{12}$ into the same cluster before merging points from $B_1$ and $B_2$ together. In this case, there exists no pruning of the tree which respects the clustering $\mc C_{\mc S}$.
\end{proof}

\begin{figure}[!ht]
\input{../figures/lbdFig2}
\caption{$\mc X \subseteq \mathbb{R}$ such that no algorithm can capture all the $(\alpha, \eta)$-center $t$-min nice subsets of $\mc X$}
\label{fig:noalgalphacp}
\end{figure}

\begin{theorem}
\label{thm:noalgalphacp}
Given the number of clusters $k$ and parameters $\alpha$, $\eta$ and $t$. For any algorithm which outputs a clustering tree $T$ of a set $\mc X$, there exists a clustering instance $(\mc X, d)$ with the following property. There exists $\mc S \subseteq \mc X$ and there exists clustering $\mc C_{\mc S} = \{S_1, \ldots, S_k\}$ induced by centers $s_1, \ldots, s_k \in \mc X$ which satisfy $(\alpha, \eta)$-center proximity w.r.t $\mc X, \mc S$ and $k$ and $ \min_i|S_i| = t \ge 2$, such that the following holds. 

If $\alpha \le 2 + \sqrt 3$, $\eta \le 1$ and , then there doesn't exist a pruning of the tree such that the corresponding clustering $\mc C_{\mc X}$ respects $\mc C_{\mc S}$.
\end{theorem}

\begin{proof}
Define $\mc X \subseteq \mathbb{R}$ as follows. Let $t_1 = \frac{t}{2}+1$ and $t_2 = \frac{t}{2}-2$. Let $O_1 = P_{t_1}(\alpha-1)\cup P_{t_1}(3\alpha-3)$ and $O_2 = \{P_{1}(\alpha-2) \thinspace\cup\thinspace P_{1}(2\alpha-3) \thinspace\cup\thinspace P_{1}(2\alpha-1) \thinspace\cup\thinspace P_{1}(3\alpha-2)\}$ and $O_3 = P_{t_2}(0)\cup P_{t_2}(2\alpha-2)\cup P_{t_2}(4\alpha-4)$. For $3\le i\le k$, define $B_i = P_t(14\alpha-4+i\alpha)$. Now, let $\mc X = \{O_1 \cup O_2 \cup O_3\cup B_3 \cup \ldots \cup B_k\}$. The set is also shown in Fig. \ref{fig:noalgalphacp}.

Now, let $B_1 = \{P_{t_2}(0) \thinspace\cup\thinspace  P_{1}(\alpha-2) \thinspace\cup\thinspace P_{t_1}(\alpha-1) \}$ and $B_2 = \{\ P_{t_2}(2\alpha-2) \thinspace\cup\thinspace P_{1}(2\alpha-1)\thinspace\cup\thinspace P_{t_1}(3\alpha-3)\}$. Also, let $B_1' = \{P_{t_1}(\alpha-1) \thinspace\cup\thinspace  P_{1}(2\alpha-3) \thinspace\cup\thinspace P_{t_2}(2\alpha-2) \}$ and $B_2' = \{\ P_{t_1}(3\alpha-3) \thinspace\cup\thinspace P_{1}(3\alpha-2)\thinspace\cup\thinspace P_{t_2}(4\alpha-4)\}$. 

Now, define $\mc S = \{B_1 \cup B_2 \cup B_3\ldots \cup B_k\} \subseteq \mc X$ and $\mc S' = \{B_1' \cup B_2' \cup B_3\ldots \cup B_k\} \subseteq \mc X$. It is easy to see that for $\alpha \le 2+\sqrt{3}$, both $\mc S$ and $\mc S'$ are $(\alpha, 1)$-center $t$-min nice and the clusterings $\mc C_{\mc S} = \{B_1, B_2, B_3, \ldots, B_k\}$ and $\mc C_{\mc S'} = \{\ B_1', B_2', B_3, \ldots, B_k\}$ satisfy $(\alpha, 1)$-center proximity. However, we will show that no algorithm can output a tree which respects both $\mc C_{\mc S}$ and $\mc C_{\mc S'}$.

For the sake of contradiction assume that there exists a tree $T$ and prunings $P$ and $P'$ such that $P$ respects $\mc C_{\mc S}$ and $P'$ respects $\mc C_{\mc S'}$. Hence, there exists a node $N_1$ such that $B_1 \subseteq N_1$ and $N_1 \cap B_2 = \phi$ and there exists $N_2$ such that $B_2 \subseteq N_2$ and $N_2 \cap B_1 = \phi$. Also, there exists node $N_1'$ such that such that $B_1' \subseteq N_1'$ and $N_1' \cap B_2' = \phi$. Now, observe that $B_1'$ contains points from both $B_1$ and $B_2$. Hence, both $N_1 \subseteq N_1'$ and $N_2 \subseteq N_1'$. Thus, $P_{t'}(3\alpha-3)\in B_2'\cap B_2 \subseteq B_2' \cap N_2 \subseteq B_2' \cap N_1'$. This contradicts the fact that $N_1' \cap B_2' = \phi$. Hence, prunings $P$ and $P'$ can not exist.
\end{proof}

\begin{figure}[!ht]
\input{../figures/lbdFig3}
\caption{$\mc X \subseteq \mathbb{R}$ such that no algorithm can capture all the $\alpha$-center $t$-min nice subsets of $\mc X$}
\label{fig:nosparsealg}
\end{figure}

\begin{theorem}
\label{thm:nosparsealg}
Given the number of clusters $k$ and a parameters $\alpha$ and $t$. For any algorithm which outputs a clustering tree $T$ of $\mc X$, there exists a clustering instance $(\mc X, d)$ with the following property. There exists $\mc S \subseteq \mc X$ and there exists clustering $\mc C_{\mc S} = \{S_1, \ldots, S_k\}$ induced by centers $s_1, \ldots, s_k \in \mc X$ which satisfies $\alpha$-center proximity w.r.t $\mc S$ and $k$ and $ \min_i|S_i| = t \ge 2$, such that the following holds. 

If $\alpha < 2\sqrt 2 + 3$ and $|\mc X \setminus \mc S| \ge \frac{3t}{2}+5$, then there doesn't exist a pruning of the tree such that the corresponding clustering $\mc C_{\mc X}$ respects $\mc C_{\mc S}$.
\end{theorem}

\begin{proof}
Define $\mc X \subseteq \mathbb{R}$ as follows. Let $t' = \frac{t}{2}-1$. Let $O_1 = \cup_{i=0}^6 P_{t'}(i\alpha-i)$ and $O_2 = \cup_{i=1}^3 P_2(i\alpha-i-2) \cup_{i=3}^5 P_2(i\alpha-i+2)$. For $3\le i\le k$, define $B_i = B(16\alpha-6+i\alpha, 0)$. Now, let $\mc X = \{O_1 \cup O_2 \cup B_3 \cup \ldots \cup B_k\}$. The set is also shown in Fig. \ref{fig:nosparsealg}.

Now, let $B_1 = \{P_{t'}(0) \thinspace\cup\thinspace  P_{2}(\alpha-3) \thinspace\cup\thinspace P_{t'}(\alpha-1) \}$ and $B_2 = \{\ P_{t'}(3\alpha-3) \thinspace\cup\thinspace P_{2}(3\alpha-1)\thinspace\cup\thinspace P_{t'}(4\alpha-4)\}$. Also, let $B_1' = \{P_{t'}(\alpha-1) \thinspace\cup\thinspace  P_{2}(2\alpha-4) \thinspace\cup\thinspace P_{t'}(2\alpha-2) \}$ and $B_2' = \{\ P_{t'}(4\alpha-4) \thinspace\cup\thinspace P_{2}(4\alpha-2)\thinspace\cup\thinspace P_{t'}(5\alpha-5)\}$. Also, let $B_1'' = \{P_{t'}(2\alpha-2) \thinspace\cup\thinspace  P_{2}(3\alpha-5) \thinspace\cup\thinspace P_{t'}(3\alpha-3) \}$ and $B_2'' = \{\ P_{t'}(5\alpha-5) \thinspace\cup\thinspace P_{2}(5\alpha-3)\thinspace\cup\thinspace P_{t'}(6\alpha-6)\}$. 

Now, define $\mc S = \{B_1 \cup B_2 \cup B_3\ldots \cup B_k\} \subseteq \mc X$, $\mc S' = \{B_1' \cup B_2' \cup B_3\ldots \cup B_k\} \subseteq \mc X$ and $\mc S'' = \{B_1'' \cup B_2'' \cup B_3\ldots \cup B_k\} \subseteq \mc X$. It is easy to see that for $\alpha \le 2\sqrt{2} + 3$, all of $\mc S$, $\mc S'$ and $\mc S''$ are $\alpha$-center $t$-min nice and the clusterings $\mc C_{\mc S} = \{B_1, B_2, B_3, \ldots, B_k\}$, $\mc C_{\mc S'} = \{B_1', B_2', B_3, \ldots, B_k\}$ and $\mc C_{\mc S''} = \{B_1'', B_2'', B_3, \ldots, B_k\}$ satisfy $\alpha$-center proximity. However, we will show that no algorithm can output a tree which respects $\mc C_{\mc S}$, $\mc C_{\mc S'}$ and $\mc C_{\mc S''}$.

For the sake of contradiction assume that there exists a tree $T$ and prunings $P$, $P'$ and $P''$ such that $P$ respects $\mc C_{\mc S}$, $P'$ respects $\mc C_{\mc S'}$ and $P''$ respects $\mc C_{\mc S''}$. By reasoning similar to Thm. \ref{thm:noalgalphacp}, we get that there exists a node $N_1$ which contains $B_1$, $B_1'$ and $B_1''$ and doesn't any point from one of $B_2'$ or $B_2''$. By symmetry, there exists node $N_2$ which contains $B_2$, $B_2'$ and $B_2''$ and doesn't contain any point from $B_1$ or $B_1'$. Observe that $N_1 \cap N_2 \neq \phi$ hence either $N_1 \subseteq N_2$ or $N_2 \subseteq N_1$. WLOG, let $N_1 \subseteq N_2$. Then, we get that $N_2$ doesn't contain any point from $B_1$ or $B_1'$ and contains both $B_1$ and $B_1'$. This leads to a contradiction. Hence, there exists no tree which contains prunings which respect all of $\mc C_{\mc S}$, $\mc C_{\mc S'}$ and $\mc C_{\mc S''}$.
\end{proof}

\section{Center Separation}

\begin{definition}[$\lambda$-center separation]
\label{defn:lambdacs}
Given a clustering instance $(\mc X, d)$ and the number of clusters $k$. We say that a clustering $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ induced by centers $c_1, \ldots, c_k$ has $\lambda$-center separation w.r.t $\mc X$ and $k$ if the following holds. For all $i\neq j$, 
$$d(c_i, c_j) > \lambda \enspace\max_{p \in [k]} \thinspace r(C_p)$$
\end{definition}

\noindent The set $\mc X$ is {\it $\lambda$-separated} {\it $t$-min $r$-max nice} if there exist a clustering $\mc C_{\mc X}=\{C_1,\ldots,C_k\}$ which satisfies $\lambda$-center separation and $\min\limits_{i} \lvert C_i\rvert = t$ and $\max\limits_{i} r(C_i) = r$.

\begin{definition}[$(\lambda, \eta)$-center separation]
Given a clustering instance $(\mc X, d)$, the number of clusters $k$ and $\mc S \subseteq \mc X$. We say that a clustering $\mc C_{\mc S} = \{S_1, \ldots, S_k\}$ induced by centers $s_1, \ldots, s_k$ has $(\lambda, \eta)$-center separation w.r.t $\mc X, \mc S$ and $k$ if the following holds.

\begin{itemize}[nolistsep, noitemsep]
\label{defn:lambdacsnoise}	

\item[$\diamond$] {\bf $\lambda$-centre separation}: For all $i\neq j$, $\thinspace d(s_i, s_j) > \lambda \max_{p} r(S_p)$
\item[$\diamond$]{\bf $\eta$-sparse noise}: For any ball $B$ such that $c(B)\in \mathcal{X}$, if $r(B)\leq \eta\max\limits_{i} \thinspace r(S_i)$, then $|B\cap (\mc X\setminus \mc S)| < \min\limits_{i} |S_i|/2$
\end{itemize}
\end{definition}

\noindent A set $\mc S \subseteq \mc X$ is {\it $(\lambda, \eta)$-separated} {\it $t$-min $r$-max nice} if there exist a clustering $\mc C_{\mc S}=\{S_1,\ldots,S_k\}$ which satisfies $(\lambda, \eta)$-center separation,  $\min\limits_{i} \lvert S_i\rvert = t$ and $\max\limits_{i} r(S_i) = r$. Note that given $(\mc X, d), k, r$ and $t$ there can be several such $\mc S$ and several clusterings $\mc C_{\mc S}$ which satisfy these conditions.

%\noindent A clustering instance $(\mc X, d)$ satisfies $(\alpha, \eta)$-center proximity if there exist $\mc S \subseteq \mc X$ such that there exists a clustering $\mc C$ of $\mc S$ which has $(\alpha, \eta)$-center proximity w.r.t $\mc X, \mc S$ and $k$.

\subsection{Goal}
We are given a clustering instance $(\mc X, d)$, the number of clusters $k$ and parameters $r$ and $t$. Our goal is to output a clustering $\mc C_{\mc X}$ of $\mc X$ which has the following property. For every $\mc S \subseteq \mc X$ which is $(\lambda, \eta)$-separated $t$-min $r$-max nice, and every clustering $\mc C_{\mc S}$ which satisfies $(\lambda, \eta)$-center separation, the clustering $\mc C_{\mc X}$ restricted to $\mc S$ equals $\mc C_{\mc S}$. 

In the next section, we propose a clustering algorithm (Alg.\ref{alg:lambdacs}) and prove (Thm.\ref{thm:lambdacsnoise}) that our algorithm indeed achieves the above mentioned goal (under certain assumptions on the parameters $\lambda$ and $\eta$). It is important to note that our algorithm only knows $\mc X$ and has no knowledge of the set $\mc S$. The output of our algorithm is a clustering that is able to capture all $(\lambda, \eta)$-separated $t$-min $r$-max nice subsets $\mc S$. More precisely, the clustering restricted to the set $\mc S$ equals $\mc C_{\mc S}$.

\subsection{Algorithm}
%Ben-David and Haghtalab studied the problem of clustering in the presence of noise under similar assumptions. They consider a data set $\mc X$ such that the optimal clustering of $\mc X$ w.r.t some center-based objective function has $\lambda$-center separation (Defn.\ref{defn:lambdacs}) except for an $\epsilon$ fraction of the points. They presented a polynomial-time algorithm that gives a $(1+O(\epsilon))$-approximation to the cost of optimal clustering. In this work, we consider a similar problem. However, our approach is different in two aspects. Just like in the case of $\alpha$-center proximity, we do not place any restriction on the size of the noise but assume that it is {\it sparse} and doesn't have any {\it structure}. Also, we don't work with any particular objective function. Our goal is to ``capture" all clusterings which have $(\lambda, \eta)$-center separation.

%Our algorithm uses a linkage-based procedure based on a novel distance condition. Before describing our algorithm, we need to define our {\it Sparse distance condition}.

Intuitively, Alg.\ref{alg:lambdacs} works as follows. In the first phase, it constructs a list of balls which have radius atmost $r$ and contain atleast $t$ points. It then constructs a graph as follows. Each ball found in the first phase is represented by a vertex. If two balls have a `large' intersection then their is an edge between the corresponding vertices in the graph. We then find the connected components in the graph which correspond to the clustering of the origin set $\mc X$. We will show that for all $\mc S \subseteq \mc X$ and for all clusterings $\mc C_S$ which have $(\lambda, \eta)$-center separation and $\min\limits_{C_i \in {\mc C}_{\mc S}} |C_i| = t$ and $\max\limits_{C_i \in {\mc C}_{\mc S}} r(C_i) = r$, Alg. \ref{alg:alphacp} outputs a clustering ${\mc C}_{\mc X}$ such that $\mc C_{\mc X}$ restricted to $\mc S$ equals $\mc C_S$. %which {\color{red}[}respects the clustering $\mc C$ {\color{red}] What does "respect" mean here?}.


\begin{algorithm}[!ht]
	\KwIn{$(\mc X, d), k, t$ and $r$}
	\KwOut{A $k$-clustering $C$ of the set $\mc X$.}
	\textbf{Phase 1}
	Let $\mc L$ denote the list of balls found so far. Initialize $\mc L$ to be the empty set. $\mc L = \phi$.
	
	Iterate over all pairs of points $p, q \in \mc X$ in increasing order of the distance $d(p, q)$. Let $B := B(p, d(p, q))$. If $|B| \ge t$ and $r(B) \le r$ then
	\begin{itemize}
	\renewcommand\labelitemi{}
		\item $\mc L = \mc L \thinspace\cup B$
	\end{itemize}
	
	Output the list of balls $\mc L = \{B_1, \ldots, B_l\}$ to the second phase of the algorithm.\\
	
	\textbf{Phase 2}
	Construct a graph $G = (V, E)$ as follows. $V = \{v_1, v_2, \ldots, v_l\}$. If $|B_i \cap B_j| \ge t/2$ then construct an edge between $v_i$ and $v_j$.
	
	Find $k$ connected components ($G_1, \ldots, G_{k}$) in the graph $G$. Construct a clustering $\mc C = \{C_1, \ldots, C_k\}$ of the set $\mc X$ as follows.
	\begin{itemize}
	\renewcommand\labelitemi{}
		\item for $m \in 1$ to $k$
		\begin{itemize}
			\renewcommand\labelitemii{}
			\item $C_m = \phi$
			\item for all $v_n \in G_m$
			\begin{itemize}
				\renewcommand\labelitemiii{}
				\item $C_m = C_m \cup B_n$
			\end{itemize}
		\end{itemize}
	\end{itemize}
	
	This step basically merges all the points in the same connected component together. After this step all the points in $\mc B = \cup_i B_i$ have been clustered.
	
	Assign $x \in \mc X \setminus \mc B$ to the cluster $C_i$ where $i := \argmin\limits_{j\in [k]} \min\limits_{y \in C_j}d(x, y)$. That is, assign $x$ to the closest cluster $C_i$. Output the clustering $\mc C = \{C_1, \ldots, C_k\}$. 
\caption{Alg. for $(\lambda, \eta)$-center separation with parameters $t$ and $r$}
\label{alg:lambdacs}
\end{algorithm}

\begin{theorem}
\label{thm:lambdacsnoise}
Given a clustering instance $(\mc X, d)$, the number of clusters $k$ and parameters $r$ and $t$. For all $\mc S \subseteq \mc X$ and for all clusterings $\mc C^*_{\mc S} = \{S_1^*, \ldots, S_k^*\}$ induced by centers $s_1, \ldots, s_k \in \mc X$ which satisfy $(\lambda, \eta)$-center separation w.r.t $\mc X, \mc S$ and $k$ such that $ \min_i|S_i^*| = t$ and $\max_i r(S_i^*) = r$, the following holds.

If $\lambda \ge 4$ and $\eta \ge 1$, then Alg. \ref{alg:lambdacs} outputs a clustering $\mc C_{\mc X}$ such that $\mc C_{\mc X}|_{\mc S} = \mc C_{\mc S}^*$ that is the clustering retricted to the set $S$ is same as the clustering $\mc C_{\mc S}^*$.
\end{theorem}
\begin{proof}
Fix $\mc S \subseteq \mc X$. Let $\mc C_{\mc S}^* = \{S_1^*, \ldots, S_k^*\}$ be a clustering of $\mc S$ such that $\min |S_i^*| = t$ and $\max_i r(S_i^*) = r$. Also, denote by $r_i := r(S_i^*)$. Let $\mc C_{\mc X} = \{C_1, \ldots, C_k\}$ be the clustering outputed by the algorithm. Throughout the proof, we will sometimes refer to $S_i^*$ as the `good clusters' and the set $\mc S$ as the good set.

Let $\mc L = \{B_1, \ldots, B_l\}$ be the list of balls as outputed by Phase 1 of Alg. \ref{alg:lambdacs}. Let $G$ be the graph as constructed in Phase 2 of the algorithm. First observe that $s_i \in \mc X$ and $r_i \le r$. Hence, $B = B(s_i, r_i) \in \mc L$ and $B = S_i^*$. WLOG, denote this ball by $B^{(i)}$ and the corresponding vertex in the graph $G$ by $v^{(i)}$. We will prove the theorem by proving two key facts.  

\begin{enumerate}[nolistsep, noitemsep, label=\textbf{F.\arabic*}]
\renewcommand\labelitemi{$\diamond$}
\item \label{fact:lambda1} If two balls $B_{i1}$ and $B_{i2}$ intersect the same good cluster $S_i^*$ then the corresponding vertices $v_{i1}$ and $v_{i2}$ are connected by a path in $G$.
\item \label{fact:lambda2} If a ball $B_{i1}$ intersects a good cluster $S_i^*$ and another ball $B_{j1}$ intersects some other good cluster $S_j^*$ then the corresponding vertices $v_{i1}$ and $v_{j1}$ are disconnected in $G$.	
\end{enumerate}
Note that the theorem follows from these two facts. We now state both of these facts formally below and prove them.\\

\noindent\textit{\underline{Proof of Fact \ref{fact:lambda1}}}
\begin{claim}
\label{claim:lambda1}
Let $\mc L, G, B^{(i)}$ and $v^{(i)}$ be as defined above. Let balls $B_{i1}, B_{i2} \in \mc L$ be such that $B_{i1} \cap S_i^* \neq \phi$ and $B_{i2} \cap S_i^* \neq \phi$. Then there exists a path between vertices $v_{i1}$ and $v_{i2}$ in the graph $G$.
\end{claim}
\vspace{-0.1in} For the sake of contradiction, assume that $v_{i1}$ and $v^{(i)}$ are not connected by an edge in $G$. Hence, $|B_{i1} \cap B^{(i)}| < t/2 \implies |B_{i1} \setminus B^{(i)}| \ge t/2$. Now, observe that since $\lambda > 4$, for all $j \neq i$, $B_{i1} \cap S_j^* = \phi$. Thus, $B_{i1} \setminus B^{(i)} \subseteq \mc X \setminus \mc S$. Hence, we get that $|B_{i1} \cap \{\mc X \setminus \mc S\}| \ge t/2$. This contradicts the sparseness assumption. Hence, $v_{i1}$ is connected to $v^{(i)}$ by an edge in $G$. By symmetry, there exists an edge between $v_{i2}$ and $v^{(i)}$. Thus, $v_{i1}$ and $v_{i2}$ are connected in $G$.\\

\noindent\textit{\underline{Proof of Fact \ref{fact:lambda2}}}
\begin{claim}
Let the framework be as in Claim \ref{claim:lambda1}. Let $B_{i1} \in \mc L$ be such that $B_{i1} \cap S_i^* \neq \phi$ and $B_{j1}$ be such that $B_{j1} \cap S_j^* \neq \phi$. Then the vertices $v_{i1}$ and $v_{j1}$ are disconnected in $G$.
\end{claim}
\vspace{-0.1in} Now, we will show that if a ball $B_{i1}$ intersects $S_i^*$ and $B_{j1}$ intersects $S_j^*$ then the corresponding vertices $v_{i1}$ and $v_{j1}$ are disconnected in $G$. For the sake of contradiction, assume that $v_{i1}$ and $v_{j1}$ are connected by a path in $G$. Hence, there exists vertices $v_{i}$ and $v_{n}$ such that $v_i$ and $v_n$ are connected by an edge in $G$ and $B_i \cap S_i^* \neq \phi$ and $B_n \cap S_n^* \neq \phi$ for some $n \neq i$. $v_i$ and $v_n$ are connected, hence $|B_i \cap B_n| \ge t/2$. Now, $\lambda \ge 4$, thus $B_i \cap \{\mc S \setminus S_i^*\} = \phi$ and $B_n \cap \{\mc S\setminus S_n^*\} = \phi$. Thus, $B_i \cap B_n \subseteq \mc X \setminus \mc S$. Thus $|B_{i} \cap \{\mc X \setminus \mc S\}| \ge t/2$. This contradicts the sparseness assumption. Hence, we get that $v_{i1}$ and $v_{j1}$ are disconnected in $G$.
\end{proof}


\begin{theorem}
Given clustering instance $(\mc X, d)$, $k$ and parameters $r$ and $t$. Algorithm \ref{alg:lambdacs} runs in time which is polynomial in $|\mc X|$.
\end{theorem}

\begin{proof}
Let $n = |\mc X|$. Phase 1 of Alg. \ref{alg:lambdacs} runs in $O(n^2)$ time. Phase 2 gets as input a list of size $l$. Constructing the graph takes $O(l^2)$ time. Finding $k$ connected components also takes $O(l^2)$ time. The remaining steps take $O(n)$ time. Hence, the algorithm runs in $O(n^2 + l^2)$ time where $l = n/t$. Hence, the total running time is $O(n^2)$.
\end{proof}

\subsection{Lower Bounds}
Recall the framework in the previous section. We were given a clustering instance $(\mc X, d)$, the number of clusters $k$ and parameters $r$ and $t$. Our goal was to output a hierarchical clustering $\mc C_{\mc X}$ which has the following property. ``{\it For every $\mc S \subseteq \mc X$ which is $(\lambda, \eta)$-separated $t$-min $r$-max nice, and every clustering $\mc C_{\mc S}$ which satisfies $(\lambda, \eta)$-center separation, $\mc C_{\mc X}$ restricted to $\mc S$ equals $\mc C_{\mc S}$.}'' Whenever the clustering has this property, we will say that it {\it captures} all $(\lambda, \eta)$-center $t$-min $r$-max nice subsets of $\mc X$.

We proposed a polynomial-time algorithm (Alg. \ref{alg:lambdacs}) and proved that for $\lambda \ge 4$ and $\eta \ge 1$, our algorithm achieves the desired objective. Another natural question to ask is that ``under what conditions or values of $\lambda$ and $\eta$ is it not possible to achieve our goal?'' We first describe our lower bound results informally.

\begin{enumerate}[nolistsep, noitemsep, label=\textbf{L.\arabic*}]
\renewcommand\labelitemi{$\diamond$}
\item \label{lowerBd:lambdacs1} If $\lambda < 4$ and $\eta \le 1$ (noise is sparse), then no algorithm can output a clustering which captures all $(\lambda, \eta)$-center $t$-min $r$-max nice subsets of $\mc X$.
\item \label{lowerBd:lambdacs2} If $\lambda < 6$ and the number of noisy points $ > 3t/2+5$, then no algorithm can output a clustering which captures all $\lambda$-center $t$-min $r$-max nice subsets of $\mc X$. Note that in this case, we do not assume that the noise is sparse.
\end{enumerate}

\noindent The proof of the lower bounds is similar to the lower bound results for center proximity. The notation used is similar to the notation of Sec. \ref{sec:alphaLBd}. We describe it here again for completeness. Given the number of clusters $k$ and parameter $t$. Denote by $P_{i}(c) := B(c, 0)$ a ball of radius $0$ centered at $c$ such that $|B(c, 0)| = i$. We will sometimes omit $c$ and use the notation $P_i$ when the center is clear from the context. 

\begin{figure}[!ht]
\input{../figures/lambdaLBDFig1}
\caption{$\mc X \subseteq \mathbb{R}$ such that no algorithm can capture all the $(\lambda, \eta)$-separated $t$-min $r$-max nice subsets of $\mc X$}
\label{fig:noalglambdacs}
\end{figure}

\begin{theorem}
Given the number of clusters $k$ and parameters $\lambda$, $\eta$, $r$ and $t$. For any algorithm which outputs a clustering a set $\mc X$, there exists a clustering instance $(\mc X, d)$ with the following property. There exists $\mc S \subseteq \mc X$ and there exists clustering $\mc C_{\mc S} = \{S_1, \ldots, S_k\}$ induced by centers $s_1, \ldots, s_k \in \mc X$ which satisfies $(\lambda, \eta)$-center separation w.r.t $\mc X, \mc S$ and $k$ and $\min_i |S_i| = t$ and $\max r(S_i) = r$ such that the following holds.

If $\lambda < 4$ and $\eta \le 1$, then $\mc C_{\mc X}|_{\mc S} \neq \mc C_{\mc S}$, i.e., $\mc C_{\mc X}$ restricted to the set $\mc S$ doesn't equal $\mc C_{\mc S}$.
\end{theorem}

\begin{proof}
Define $\mc X \subseteq \mathbb{R}$ as follows. Let $t_1 = \frac{t}{2}+1$ and $t_2 = \frac{t}{2}-2$. Let $O_1 = \cup_{i=0}^2 P_{t_2}(4ir)$ and $O_2 = \cup_{i=0}^1 P_{t_1}(2r+4ir)$ and $O_3 = \cup_{i=0}^3 P_{t_1}(r+2ir)$. For $3\le i\le k$, define $B_i = P_t(18r+ir)$. Now, let $\mc X = O_1 \cup O_2 \cup O_3 \cup B_3 \cup \ldots \cup B_k$. The set is also shown in Fig. \ref{fig:noalglambdacs}.

Now, let $B_1 = \{P_{t_2}(0) \thinspace\cup\thinspace  P_{1}(r) \thinspace\cup\thinspace P_{t_1}(2r) \}$ and $B_2 = \{\ P_{t_2}(4r) \thinspace\cup\thinspace P_{1}(5r)\thinspace\cup\thinspace P_{t_1}(6r)\}$. Also, let $B_1' = \{P_{t_1}(2r) \thinspace\cup\thinspace  P_{1}(3r) \thinspace\cup\thinspace P_{t_2}(4r) \}$ and $B_2' = \{\ P_{t_1}(6r) \thinspace\cup\thinspace P_{1}(7r)\thinspace\cup\thinspace P_{t_2}(8r)\}$.

Now, define $\mc S = \{B_1 \cup B_2 \cup B_3\ldots \cup B_k\} \subseteq \mc X$ and $\mc S' = \{B_1' \cup B_2' \cup B_3\ldots \cup B_k\} \subseteq \mc X$. It is easy to see that for $\lambda < 4$, both $\mc S$ and $\mc S'$ are $(\lambda, 1)$-separated $t$-min $r$-max nice and the clusterings $\mc C_{\mc S} = \{B_1, B_2, B_3, \ldots, B_k\}$ and $\mc C_{\mc S'} = \{\ B_1', B_2', B_3, \ldots, B_k\}$ satisfy $(\lambda, 1)$-center proximity. 

We will show that no algorithm can output a clustering $\mc C_{\mc X}$ such that $\mc C_{\mc X}|_{\mc S} = \mc C_{\mc S}$ and $\mc C_{\mc X}|_{\mc S'} = \mc C_{\mc S'}$. For the sake of contradiction assume that this is indeed the case. There exists cluster $C_1' \in \mc C_{\mc X}$ such that such that $B_1' \subseteq C_1'$ and $C_1' \cap B_2' = \phi$. Now, observe that $B_1'$ contains points from both $B_1$ and $B_2$. Hence, there exists a cluster $C' \in \mc C_{\mc X}$ such that $C' \cap B_1 \neq \phi$ and $C' \cap B_2 \neq \phi$. This is a contradiction.
\end{proof}

\begin{figure}[!ht]
\input{../figures/lambdaLBDFig2}
\caption{$\mc X \subseteq \mathbb{R}$ such that no algorithm can capture all the $\lambda$-separated $t$-min $r$-max nice subsets of $\mc X$}
\label{fig:nosparsealglambdacs}
\end{figure}

\begin{theorem}
Given the number of clusters $k$ and parameters $\lambda$, $r$ and $t$. For any algorithm which outputs a clustering a set $\mc X$, there exists a clustering instance $(\mc X, d)$ with the following property. There exists $\mc S \subseteq \mc X$ and there exists clustering $\mc C_{\mc S} = \{S_1, \ldots, S_k\}$ induced by centers $s_1, \ldots, s_k \in \mc X$ which satisfies $\lambda$-center separation and $\min_i |S_i| = t$ and $\max r(S_i) = r$ such that the following holds.

If $\lambda < 6$ and $|\mc X\setminus \mc S|\ge \frac{3t}{2}+5$, then $\mc C_{\mc X}|_{\mc S} \neq \mc C_{\mc S}$, i.e., $\mc C_{\mc X}$ restricted to the set $\mc S$ doesn't equal $\mc C_{\mc S}$.
\end{theorem}

\begin{proof}
Define $\mc X \subseteq \mathbb{R}$ as follows. Let $t' = \frac{t}{2}-1$. Let $O_1 = \cup_{i=0}^6 P_{t'}(2ir)$ and $O_2 = \cup_{i=0}^5 P_2(r+2ir)$. For $3\le i\le k$, define $B_i = P_t(22r+ir)$. Now, let $\mc X = \{O_1 \cup O_2 \cup B_3 \cup \ldots \cup B_k\}$. The set is also shown in Fig. \ref{fig:nosparsealg}.

Now, let $B_1 = \{P_{t'}(0) \thinspace\cup\thinspace  P_{2}(r) \thinspace\cup\thinspace P_{t'}(2r) \}$ and $B_2 = \{\ P_{t'}(6r) \thinspace\cup\thinspace P_{2}(7r)\thinspace\cup\thinspace P_{t'}(8r)\}$. Also, let $B_1' = \{P_{t'}(2r) \thinspace\cup\thinspace  P_{2}(3r) \thinspace\cup\thinspace P_{t'}(4r) \}$ and $B_2' = \{\ P_{t'}(8r) \thinspace\cup\thinspace P_{2}(9r)\thinspace\cup\thinspace P_{t'}(10r)\}$. Also, let $B_1'' = \{P_{t'}(4r) \thinspace\cup\thinspace  P_{2}(5r) \thinspace\cup\thinspace P_{t'}(6r) \}$ and $B_2'' = \{\ P_{t'}(10r) \thinspace\cup\thinspace P_{2}(11r)\thinspace\cup\thinspace P_{t'}(12r)\}$. 

Now, define $\mc S = \{B_1 \cup B_2 \cup B_3\ldots \cup B_k\} \subseteq \mc X$, $\mc S' = \{B_1' \cup B_2' \cup B_3\ldots \cup B_k\} \subseteq \mc X$ and $\mc S'' = \{B_1'' \cup B_2'' \cup B_3\ldots \cup B_k\} \subseteq \mc X$. It is easy to see that for $\lambda < 6$, all of $\mc S$, $\mc S'$ and $\mc S''$ are $\lambda$-separated $t$-min $r$-max nice and the clusterings $\mc C_{\mc S} = \{B_1, B_2, B_3, \ldots, B_k\}$, $\mc C_{\mc S'} = \{B_1', B_2', B_3, \ldots, B_k\}$ and $\mc C_{\mc S''} = \{B_1'', B_2'', B_3, \ldots, B_k\}$ satisfy $\lambda$-center separation.

$\mc C_{\mc X}$ such that $\mc C_{\mc X}|_{\mc S} = \mc C_{\mc S}$ and $\mc C_{\mc X}|_{\mc S'} = \mc C_{\mc S'}$ and $\mc C_{\mc X}|_{\mc S''} = \mc C_{\mc S''}$. For the sake of contradiction assume that this is the case. There exists cluster $C_1 \in \mc C_{\mc X}$ such that such that $B_1 \subseteq C_1$ and $C_1 \cap B_2 = \phi$. Similarly, there exists cluster $C_1' \in \mc C_{\mc X}$ such that such that $B_1' \subseteq C_1'$ and $C_1' \cap B_2' = \phi$ and there exists cluster $C_1'' \in \mc C_{\mc X}$ such that such that $B_1'' \subseteq C_1''$ and $C_1'' \cap B_2'' = \phi$. Now, $C_1, C_1', C_1'' \in \mc X$ and $C_1 \cap C_1' \neq \phi$ and $C_1'' \cap C_1' \neq \phi$. Hence, $C_1 = C_1' = C_1''$. Hence, $B_1 \cup B_1' \cup B_1'' \subseteq C_1$. Observe, that $B_1'' \cap B_2 \neq \phi$. Hence, $C_1\cap B_2 \neq \phi$. This is a contradiction.
\end{proof}


\bibliographystyle{alpha}
\bibliography{colt2016ClusteringNoise} 
\end{document}
