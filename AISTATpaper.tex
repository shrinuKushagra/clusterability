\documentclass[twoside]{article}
\usepackage{aistats2016}
%\usepackage[paper]{nickstyle}

\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb, amsmath, amsfonts, amsthm}
\usepackage{thmtools}
\usepackage{algorithm} 
\usepackage{enumitem}

\newcommand{\mc}{\mathcal}
\setlength{\parindent}{24pt}
\renewcommand{\bar}[1]{\overline{#1}} 

\newtheorem{theorem}{Theorem}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}

\newcommand{\ConjName}[1]{\label{con:#1}}
\newcommand{\Conj}[1]{Conjecture~\ref{con:#1}}
\newcommand{\ProblemName}[1]{\label{prob:#1}}
\newcommand{\Problem}[1]{Problem~\ref{prob:#1}}
\renewcommand{\dot}{\bullet}
\newcommand{\Tr}{\operatorname{tr}}
\newcommand{\eps}{\epsilon}

\newcommand{\lmax}{\lambda_\mathrm{max}}
\newcommand{\lmin}{\lambda_\mathrm{min}}
\newcommand{\ufinal}{u_\mathrm{final}}
\newcommand{\lfinal}{l_\mathrm{final}}
\newcommand{\umax}{u_\mathrm{max}}

\newcommand{\Symraw}{\mathbb{S}}
\newcommand{\Sym}[1][]{\Symraw^{\ifthenelse{\equal{#1}{}}{m}{#1}}}
\newcommand{\Psd}[1][]{\Symraw_+^{\ifthenelse{\equal{#1}{}}{m}{#1}}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iprod}[2]{\langle #1, #2 \rangle}
\newcommand{\paren}[2][]{#1({#2}#1)}
\newcommand{\qform}[2]{\transp{#2}#1#2}
\newcommand{\transp}[1]{#1^T}



% Simple (outer) environment for algorithms
\newenvironment{outer_alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
        \setlength{\leftmargin}{5pt}
    }
}
{
    \end{list}
}

% Simple environments for algorithms
\newenvironment{alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
    }
}
{
    \end{list}
}

\DeclareMathOperator{\argmin}{argmin}

%%%%% Title %%%%%
\title{\LARGE Clusterability with structural noise}
\author{}


%%%%% Document Body %%%%%
\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Clustering is the process of partitioning a given data set such that objects in the same partition are similar to one another while objects in different partitions are different. The most common clustering methods are often either linkage-based or center-based. In linkage based clustering, the goal is not to find a single partition but a hiearchy of partitions represented using a clustering tree (hierarchical clustering). These methods start by having each point in its own cluster. Then, two closest (or most similar) clusters are merged together using some similarity measure until the whole data is just a single cluster. One of the common drawbacks of these methods is that they are not robust to noise \cite{narasimhan2005q}. 

Center-based clustering methods work with an optimization function. The objective is to find the set of centers which minimize this function. It is known that most of commonly-used objective functions are NP-Hard to optimize. Still, center-based clustering algorithms are routinely and successfully employed in practice.

Hence, to improve the theoretical understanding of clustering and to explain the apparent gap between theory and practice, various notions of clustering have been proposed. The intuition is that real-world data is not `wort-case' and often enjoy some nice properties which are exploited by the clustering algorithms. In this regard, the \emph{CDNM} hypothesis has become quite popular. It says that ``Clustering is difficult only when it does not matter" \cite{daniely2012clustering}.    

Each of the different notions of clustering that have been proposed, attempt to capture some nice property of the data. The results generally follow one of the two directions. One category of results show that under the proposed clusterability notion some known center-based algorithm runs in polynomial time. The other category of results propose some new clustering algorithm and show that the proposed algorithm runs in polynomial time if the data satisfies the niceness conditions. Now, we will survey each of the known clusterability notions and the known results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Outline}
This paper is organized as follows. In section \ref{sec:previous} we will talk about related work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Previous Work}
\label{sec:previous}

Various notions of clusterability has been proposed in the past. The majority of work focus on the fact that clustering can be done efficiently on the data sets that follow some nice structure i.e. they are well clusterable.

\subsection{Clusterability Assumptions}


Bilu et al. \cite{bilu2012stable} consider an input of an optimization problem to be \emph{stable} if small perturbations of the input does not change the optimal solution. Specially, they give an efficient algorithm to find the max-cut of sufficiently stable graphs.
In the same direction, \cite{awasthi2012center, balcan2012clustering}, consider similar condition on the input of center-based clustering tasks, and propose efficient algorithms that outputs the optimal clustering of the data. Another natural assumption \cite{ackerman2009clusterability} on the data is that cost of optimal clustering should not change considerably if the centers of optimal clusters are slightly perturbed. Given this condition, Ackerman et al. design an efficient clustering algorithm that outputs a clustering with near-optimal cost. 

%The last clusterability assumption that we want to mention is called {$\alpha$-Center Proximity\cite{awasthi2012center}} and is similar in nature to the condition that we suggest. The definition basically says that a point is $\alpha$-times closer to its own center than to any other center. 
%
%
%{$(\epsilon, \delta)$-Center Pertubation }
%\cite{ackerman2009clusterability}
%\begin{definition}
%Given a data set $\mc X$. We say that two clusterings $\mc C$ and $\mc C'$ are $\epsilon$-close, if there exists centers $c_1,\ldots, c_k \in \mc C$ and centers $c_1',\ldots, c_k' \in \mc C'$ such that $\|c_i - c_i'\| \le \epsilon$. $\mc X$ is $(\epsilon, \delta)$-CP clusterable, if for every clustering $\mc C$ of $\mc X$ that is $\epsilon$-close to some optimal $k$-clustering of $\mc X$, $Cost(\mc C) \le (1 + \delta) \thinspace Cost(OPT(\mc X))$. 
%\end{definition}
%Ben-David et. al \cite{ackerman2009clusterability} give an algorithm and prove that if $\mc X$ is $(\epsilon, \delta)$-CP clusterable then the algorithm runs in $O(kn^{k/\epsilon^2})$ and outputs a clustering $\mc C$ such that the cost of $Cost(\mc C) \le (1+\delta)\thinspace Cost(OPT(\mc X))$. The authors also refer to this notion as \textbf{Additive Pertubation Robustness (APR)} \cite{ben2015computational}.  
%
%\subsection*{$\alpha$-Pertubation Resilience \cite{bilu2012stable}}
%\begin{definition}
%Given $(\mc X, d)$ where $\mc X$ is the data set and $d$ is a distance function. We say that $d'$ is an $\alpha$-pertubation of $d$ if for every $x,y \in \mc X$, $d(x,y) \le d'(x,y) \le \alpha d(x,y)$. $\mc X$ has $\alpha$-PR if there exists an optimal clustering $\mc C$ for $(\mc X, d)$ such that for every $d'$ which is an $\alpha$-pertubation of $d$, $\mc C$ is also an optimal clustering for $(\mc X, d')$.
%\end{definition}
%The definition basically says that an optimal clustering remains optimal for small multiplicative pertubation of the input. Bilu and Linial \cite{bilu2012stable} consider the problem of max-cut clustering of a graph. They give an algorithm for this problem and prove that it runs in polynomial-time provided $\alpha > \min \{\sqrt{n\Delta}, n/2\}$ where $\Delta$ is the maximum degree of the graph.
%
%Awasthi et. al \cite{awasthi2012center} consider the problem of clustering with center-based objective functions. They give an algorithm based on single-linkage heurestic and prove that it runs in polynomial time if $\alpha \ge 3$. Balcan et. al \cite{balcan2012clustering} make progress on the same problem and give a polynomial-time algorithm for $\alpha \ge 1+\sqrt{2}$. 

%\subsubsection*{$\alpha$-Center Proximity \cite{awasthi2012center}}
%\begin{definition}
%Given $(\mc X, d)$. Let $\mc C = \{c_1, \ldots, c_k\}$ be a clustering of $\mc X$. We say that $\mc X$ has $\alpha$-center proximity w.r.t $\mc C$, if for all $i \le k$ and for all $x \in C_i$, $d(x,c_i) < \alpha d(x, c_j)$ where $j \neq i$.   
%\end{definition}
%The definition basically says that a point is $\alpha$-times closer to its own center than to any other center.  Awasthi et. al \cite{awasthi2012center} showed that center proximity is a weaker notion than pertubation resilience. That is, if an instance is $\alpha$-pertubation resilient then it is also has $\alpha$-center proximity. Both \cite{awasthi2012center} and \cite{balcan2012clustering} essentially work with $\alpha$-center proximity. The former shows gives a polynomial-time algorithm for $\alpha \ge 3$ and then uses this result to show that this algorithm is also pertubation resilient. The latter does the same for $\alpha \ge 1 + \sqrt{2}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clusterability with noise}
All the clusterability notions discussed so far assume that the entire data set enjoys some nice property. This can be unreasonable in the real-life situations. A more realistic scenario would be the one in which most of the input data have a given nice structure but the remaining points violate the structure. The fraction of points that disrigard the niceness property is referred to as noise.
%(for eg. $\alpha$-pertubation resilience)
In this section, we discuss notions of clusterability which take noise into account. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\cite{balcan2012clustering} assume that a big fraction of input data is sufficiently stable and generalize their algorithm to give an approximation of the $k$-means optimal solution. Their approximation guaranty is based on the assumption that the fraction of noisy points is very small.
Balcan et. al \cite{balcan2008discriminative} consider the problem of finding the ground-truth clustering of the data. They assume that every point of the data set is closer to the points in its true cluster than the points outside. Based on this assumption, they introduce an efficient algorithm which outputs a hierarchical clustering tree. It's claimed that a pruning of this tree is provably close to the true clustering if the size of smallest true cluster is small.

Our clusterability assumption is similar in nature to the definition proposed by Ben-David et al. In \cite{ben2014clustering}, a subset of data is defined to have \emph{$\nu$-separated balls} condition, if it can be covered by $k$ balls of bounded radius which are separated by distance at least $\nu$. Given an input data set with small fraction of noise, Ben-David et al. introduce a generic algorithm for how to augment any center-based algorithm to output the underlying $k$-clustering of the un-noisy part of the data.



%\subsubsection*{$\nu$-Separated Balls \cite{ben2014clustering}}
%\begin{definition}
%Given $\mc X$. Let $\mc I \subseteq \mc X$ be such that $\mc I = \cup_{i=1}^k B_i$ where each $B_i$ is a ball such that rad$(B_i) \le r$ and for all $i \neq j$, $|c(B_i)-c(B_j)| > \nu$ where $c(B)$ denotes the center of a ball.
%\end{definition}
%The remaining part of the data $\mc N = \mc X \setminus \mc I$ denotes the noise. Ben-David et. al \cite{ben2014clustering} give a generic algorithm for how to augment any center-based algorithm into one which is robust to noise. For small noise, the robustified algorithm outputs a clustering $\mc C'$ of $\mc X$ such that $\mc C'|_{\mc I}$ is the optimal clustering of $\mc I$.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsubsection*{$(\alpha, \epsilon)$-Pertubation Resilience \cite{balcan2012clustering}}
%\begin{definition}
%Given $(\mc X, d)$. Let $\mc C$ be an optimal clustering for $(\mc X, d)$ under some objective function $\phi$. Let $d'$ be an $\alpha$-pertubation of $d$ and $\mc C'$ be an optimal clustering for $(\mc X, d')$. $\mc X$ has $(\alpha, \epsilon)$-PR if for every $d'$ which is an $\alpha$-pertubation of $d$, $\mc C'$ is $\epsilon$-close to $\mc C$.
%\end{definition}
%$(\alpha, \epsilon)$-pertubation resilience is a natural relxation of $\alpha$-pertubation resilience. Balcan et. al \cite{balcan2012clustering} obtained positive results for $(\alpha, \epsilon)$-PR when $\phi$ is the $k$-median objective function. They give a polynomial-time algorithm which produces a $O(1+\epsilon/\rho)$ ($\rho$ is fraction of points in the smallest cluster) approximation to the optimum provided $\alpha > 2 + \sqrt{7}$. The number of noisy points that they can handle for the $k$-median objective is $\le n\epsilon$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsubsection*{$\nu$-Strict Separation \cite{balcan2008discriminative}}
%\begin{definition}
%Given $(\mc X, d)$. Let $C(x)$ denote the cluster to which $x$ belongs. We say that $\mc X$ has strict separation if for all $y \in C(x)$ and for all $z \not\in C(x)$, $d(x,y) < d(x,z)$. We say that $\mc X$ has $\nu$-strict separation if there exists $\mc X' \subseteq \mc X$ such that $|\mc X\setminus \mc X'| < \nu|\mc X|$ and $\mc X'$ has strict separation.
%\end{definition}
%Balcan et. al \cite{balcan2008discriminative} show that if size of the smallest cluster is greater than $5\nu|\mc X|$ then their algorithm produces a hierarchical clustering tree such that a pruning of the tree is $\nu$-close to the ground truth clustering. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\centering
\caption{Different notions of clusterability. The type and amount of noise these notions can handle.}
\label{table:clusterabilitynotions}
\vspace{2mm}
\begin{tabular}{c|ccc|c}
\hline
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{}\\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Clusterability} \\ \textbf{Notion}\end{tabular}} &  & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Small}\\ \textbf{noise}\end{tabular}} &  & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Structural}\\ \textbf{noise}\end{tabular}} \\ 
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{}\\\hline
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{}\\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$\nu$-separated balls\\ (known radius)\\ \cite{ben2014clustering}\end{tabular}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}$O\big(\frac{1}{k+1}\big)$ \end{tabular}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{Alg. \ref{alg:NotKnown}} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} \\ 
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$\nu$-separated balls\\ (unknown radius)\end{tabular}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$??$} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{Alg. \ref{alg:Known}} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$(\alpha, \epsilon)$-pertubation \\ resilience \\\cite{balcan2012clustering}\end{tabular}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}$O(\epsilon)$\end{tabular}} & \multicolumn{1}{c|}{} & \emph{Future work} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} \\ 
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}{$\nu$-strict separation}\\\cite {balcan2008discriminative}\end{tabular}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} $O(\nu)$ \\ \end{tabular}} & &\emph{Future work} \\ \hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our Contribution}
Table \ref{table:clusterabilitynotions} shows the different notions of clusterability (in the presence of noise) that have been proposed in the literature. All the notions only deal with small quantitative noise. That is, they give efficient clustering algorithms only when there exist a small fraction of noisy points in the data.
% is small. For example, Balcan et. al \cite{balcan2012clustering} assume that the data follows $(\alpha, \epsilon)$-pertubation resilience property and they give an efficient algorithm for approximating . Their algorithm can only handle noise which is in $O(\epsilon)$. Similarly, other notions of clusterability can only handle a small amount of noise.
One of the reasons for this is that these algorithms make no additional assumption on noise other than that it should be small. However, it might make more sense to make some structural assumptions on the noise rather than its size. Intuitively, we expect that the noise is \emph{sparse}. That is, it is not `concentrated' at one place and rather spread across the given data set. In other words, we want to say that the noisy points do not pretend to be a cluster. 

In this work, we want to formalize this assumption as \emph{structural sparse noise}. We define our notion of $\nu$-separated balls with sparse noise. Under this assumption, we give two algorithms. The first algorithm deals with the case when the maximum radius of the balls is known to the algorithm. Ben David et. al \cite{ben2014clustering} work with the same assumption. The other algorithm which is slightly more involved works with the case when the maximum radius is not known. In this case, we output a small list of possible solutions such that one of the clusterings in the list is same (on the non-noisy part of data) as the target clustering. 


\section{Problem Statement}

\subsection{Notation}
For any set $B\subset \mc X$, we denote $c(B)$ as the center of $B$ which is defined as the average of points in $B$. Radius of the set $B$ is defined as $r(B)=\max_{x\in B} |x-c(B)|$. 

A $k$-clustering of the set $\mc X$ is defined as the set $\mc C = \{C_1,\ldots,C_k\}$ such that $C_i \subseteq \mc X$, $\cup C_i = \mc X$ and $C_i \cap C_j = \phi$. For any $x \in \mc X$, if $x \in C_i$, we say that $x$ lies in the $i^{th}$ cluster. Sometimes, we also denote the clustering by $\mc C = \{c_1,\ldots,c_k\}$ where $c_i \in \mc X$. For any $x \in \mc X$, $x$ lies in the cluster given by $\argmin |x-c_i|$.

\begin{definition}[Niceness assumption]
% Adding C_{k+1} as garbage collector?
Given a set $\mc X$ and $k$, we say that $\mc X$ is $(\lambda,\nu, \eta)$-nice if there exists balls $\mc B=\{B_1,\ldots,B_k\}\subset \mathcal{X}$ with the following properties.
\vspace{-3mm}
\begin{itemize}
\item{\bf{$\lambda$-separated and $\nu$-dense balls}:}
	\begin{itemize}[nolistsep,noitemsep]
	\item[$\diamond$] For all $i\neq j, |c(B_i)-c(B_j)| > \lambda\max_i \thinspace r(B_i)$
	\item[$\diamond$] $\min_i|B_i| \ge \nu |\mc X|$
	\end{itemize}
\item{\bf{$\eta$-sparse noise}}: For any ball $B$ such that $c(B)\in \mathcal{X}$, if $r(B)\leq \eta \thinspace\max_i  r(B_i)$, then 
%\vspace{-2mm}
\begin{align*}
|B\cap \{X \backslash \cup_i B_i\}| < \min_i |B_i|
\end{align*}
\end{itemize}
\label{defn:niceness}
\end{definition}

\vspace{1mm}
\begin{lemma}
\label{lemma:chknice}
Given a $(\lambda,\nu, \eta)$-nice set $\mc X$, $k$ and a set of balls $\mc B = \{B_1,\ldots,B_k\}$, we can verify if the set $\mc B$ is $\lambda$-separated, $\nu$-dense and $\eta$-sparse in time poynomial in $|\mc X|$ and $k$.
\end{lemma}
\begin{proof}
Checking if the balls are separated and dense takes $O(k^2 + |\mc X|)$. Verifying the sparseness condition takes time $O(|\mc X|^2)$.
\end{proof}

\section{Objective}
Given a $(\lambda, \nu, \eta)$-nice set $\mc X$ and $k$. Let $\mc B = \{B_1,\ldots,B_k\}$ be a set of balls which is $\lambda$-separated, $\nu$-dense and $\eta$-sparse. We say that a $k$-clustering $\mc C' =\{C_1',\ldots,C_k'\}$ of $\mc X$ is consistent with $\mc B$ if
%\vspace{-3mm}
\begin{align*}
	\mc C'|_\mc B = \{B_1,\ldots,B_k\}
\end{align*}
We consider the following different goals for a clustering algorithm. 
\begin{itemize}[noitemsep,nolistsep]
\item {\em Consistent with one} - The algorithm outputs a clustering $\mc C'$ which is consistent with some $\mc B$.
\item {\em List of solutions} - The algorithm outputs a list of clusterings $\mc L$ with the following property. For every $\mc B$, there exists $\mc C' \in \mc L$ that is consistent with $\mc B$.
\item {\em Unique solution} - The algorithm outputs a clustering $\mc C'$ which is consistent with all $\mc B$.
\end{itemize}

The goal of unique solution is very ambitious. In general, a unique solution doesn't always exist. In this section, we wish to examine conditions under which the solution is unique. If the solution is not unique we wish to output a {\em list of solutions}.  

Given any set $\mc B = \{B_1,\ldots, B_k\}$ which is $\lambda$-separated, $\nu$-dense and $\eta$-sparse. We consider two possible parametrizations of the set $\mc B$. First is the maximum radius $r = \max_i r(B_i)$. We show that for fixed $r$, it is possible to get a unique solution. The second is the number of points in the smallest ball $t = \min_i |B_i|$. In this case, we construct a list of possible solutions. Finally, we get rid of both the paramerizations and construct a list of solutions for any $\mc B$.

\subsection{Unique solution for fixed $r$}
\label{section:uniqueR}
\begin{lemma}
\label{lemma:uniqueR}
Let $\mc X$ be $(8, \nu, 2)$-nice. Let $\mc B = \{B_1,\ldots, B_k\} \subseteq \mc X$ be any set with $r = \max_i r(B_i)$ which is $8$-separated, $\nu$-dense and $2$-sparse. Then there exists a set $\mc B(r) = \{B_1',\ldots,B_k'\}$ such that the sets $\mc B(r)$ and $\mc B$ are unique under intersection. That is 
\begin{itemize}[nolistsep]
\item For all $i$, $B_i \cap B_i' \neq \phi$. 
\item For all $i\neq j$, $B_i \cap B_j' = \phi$.
\end{itemize}
\end{lemma} 
\begin{proof}
Let $B_1'$ be the densest ball of radius $2r$ such that $c(B_1') \in \mc X$. Assume for the sake of contradiction, that $B_1'$ is such that for all $i$, $B_1' \cap B_i = \phi$. Now, $B_1' \cap \{X \backslash \cup_{i\in[k]} B_i\} = B_1'$. Observe that $|B_1'| \ge \min_i |B_i|$. Thus, $|B_1' \cap \{X \backslash \cup_{i\in[k]} B_i\}| \ge \min |B_i|$ which contradicts the fact that $\mc B$ satisfies the sparseness condition. Thus, $\exists i$ s.t. $B_i \cap B_1' \neq \phi$. $\lambda \ge 8$ implies that $B_1' \cap B_j = \phi$ for all $j \neq i$. Without loss of generality, let $B_i = B_1$. 

Let $\mc X_1 = \mc X \setminus B_{4r}(c(B_1'))$. Observe that $B_1 \subseteq B_{4r}(c(B_1'))$ and hence $X_1 \cap B_1 = \phi$. Let $B_2'$ be the densest ball with $c(B_2') \in \mc X_1$. Now, using the same argument as above, we get that $B_2' \cap B_2 \neq \phi$ and $B_2' \cap B_i = \phi$ for all $i\neq 2$. Now, we define $\mc X_2 = \mc X_1 \setminus B_{4r}(c(B_2'))$. Repeating this process $k$ times gives balls $B_1', \ldots, B_k'$ with the desired properties.
\end{proof}

\begin{theorem}
\label{theorem:uniqueR}
Let $\mc X, \mc B, \mc B(r)$ be as defined in Lemma \ref{lemma:uniqueR}. Let $\mc C(r)$ be the clustering induced by the centers of $\mc B(r)$. Then, $\mc C(r)|_{\mc B} = \{B_1, \ldots, B_k\}$. 
\end{theorem}
\begin{proof}
Let $x \in B_i$. Observe that, $|c(B_i)-c(B_i')| \le 3r$ and for all $j\neq i$, $|c(B_i')-c(B_j)| > 5r$. Now, $|x-c(B_i')| \le |x-c(B_i)| + |c(B_i')-c(B_i)| \le r+3r = 4r$. For $j \neq i$, $|x-c(B_j')| \ge |c(B_j')-c(B_i)| - |x-c(B_i)| > 5r - r = 4r$. Hence, we see that the clustering $\mc C(r)$ assigns $x$ to the $i^{th}$ cluster. Hence, $\mc C(r)|_\mc B = \{B_1,\ldots,B_k\}$.
\end{proof}

Thm. \ref{theorem:uniqueR} shows that for a fixed value of $r$, $\mc C(r)$ is the unique solution for all $\mc B'$ such that $\max_i r(B_i') = r$. The proof of Lemma \ref{lemma:uniqueR} actually gives an algorithm for finding the set $\mc B(r)$ and the clustering $\mc C(r)$ which we describe next. Note that for a given $r$, if there exists a set $\mc B = \{B_1,\ldots,B_k\}$ which is $8$-separated, $\nu$-dense and $2$-sparse such that $\max r(B_i) = r$, then Alg. \ref{alg:NotKnown} finds a clustering which is consistent with $\mc B$.

\begin{algorithm}
\begin{alg}
\item[] Set $r = \max_i \thinspace r(B_i)$
\item[] for $i=1$ to $k$
	\begin{itemize}
		\item[] Let $B_i'$ be the densest ball of radius $2r$ such that $c(B_i') \in \mathcal{X}$.
		\item[] Set $\mc X=\mc X\setminus B_{4r}(c(B_i'))$. 
	\end{itemize}
\item[] Let $\mc B' := \{B_1',\ldots,B_k'\}$ and $c(\mc B') = \{c(B_1'),\ldots,c(B_k')\}$.
\item[] Output $\mc B'$ and the clustering $\mc C'$ induced by $c(\mc B')$.
\label{alg:NotKnown}
\end{alg}
\caption{Alg. for fixed $\max \thinspace r(B_i)$}
\end{algorithm}

\begin{theorem}
Algorithm \ref{alg:NotKnown} runs in time $O($poly$(|\mathcal{X}|,k))$.
\end{theorem}
\begin{proof}
During each iteration, we find the densest ball of radius $2r$. For each point in $\mathcal{X}$, we count the number of points whose distance is $\le 2r$. This can take $O(n)$ in the worst case. Hence, finding the densest ball takes $O(n^2)$ time. Similarly, deleting points from $\mathcal{X}$ takes $O(n^2)$ time. Hence, the for-loop takes $O(n^2k)$ time.
\end{proof}

\subsection{List of solutions for fixed $t$}
\label{section:structureT}
%All algoritthms are polynomial in n and k
We will first construct an example and show that for fixed $t$, a unique solution is not possible.
\begin{example}
\label{example:notuniqueT}
Let $\mc X \subseteq \mathbf{R}^2$ be the following. Let $B_1 = B((-0.9, 0.0), 0.1)$, $B_2 = B((0.9, 0.0), 0.1)$ and $B_3 = B((8.0, 0.0), 1)$ and each $B_i$ contains $t$ points. Let $\mc X = B_1 \cup B_2 \cup B_3$ and $k = 2$.

Let $\mc B = \{B_1, B_2\}$ and $\mc B' = \{B((0,0), 1), B_3\}$. It is easy to see that both $\mc B$ and $\mc B'$ are $8$-separated, $1/3$-dense and $1$-sparse. However, no clustering exists that can be consistent with both $\mc B$ and $\mc B'$.
\end{example}

\begin{lemma}
\label{lemma:structureT}
Let $\mc X$ be $(\lambda, \nu, \eta)$-nice. Let $\mc B = \{B_1,\ldots, B_k\} \subseteq \mc X$ and $\mc B' = \{B_1',\ldots, B_k'\} \subseteq \mc X$ be two sets which are $\lambda$-separated, $\nu$-dense and $\eta$-sparse. Let $t = \min_i |B_i| = \min_i |B_i'|$. WLOG, assume that $\max r(B_i) = r_1 < r_2 = \max r(B_i')$. Then, for all $i$, there exists $j$ such that $B_i \cap B_j' \neq \phi$.
\end{lemma} 
\begin{proof}
Assume that there exists $B_i$ such that for all $B_j'$, $B_i \cap B_j' = \phi$. Now, $B_i$ violates the sparseness condittion w.r.t $\mc B'$. Hence, such a ball $B_i$ cant exist. 
\end{proof}
\vspace{-2mm}
Example \ref{example:notuniqueT} and Lemma \ref{lemma:structureT} together show that although a unique solution is not possible but the solution has some structure. Namely, the set $\mc B$ with the smallest $r$ must intersect with ones which have greater $r$. 

We present an algorithm (Alg. \ref{alg:Known}) which outputs a list of clusterings $\mc L$. We will then show that for any ball $\mc B$ with $\min |B_i| = t$\footnote{Throughtout this section, we will assume that $t \ge \nu|\mc X|$.}, $\mc L$ contains a clustering which is consistent with $\mc B$. Intuitively, Alg. \ref{alg:Known} works as follows. In the first phase, we cover the set $|\mc X|$ with balls which have atleast $t$ points starting with the ball of smallest radius. In the second phase, we merge the balls found using some sort of an ``iterative variant" of single-linkage. 
\begin{algorithm}[t]
\begin{alg}
\item[] \textbf{Phase 1:}
%\begin{itemize}
\item[] Let $\mc S$ denote the balls found so far. Initialize $\mc S = \phi$.
\item[] Let $D$ be the sorted list of distances between all pairs of points in $\mc X$. $D = \{d_1,\ldots,d_t\}$. Note that $t \le |\mc X|^2/2$.
\item[] for $i=1$ to $t$
\begin{itemize}
\item[] foreach $x \in \mc X$
\begin{itemize}
\item[] Let $B$ be a ball of radius $d_i$ centered at $x$.
\item[] if $|B| \ge t$ and $|B \cap \mathcal{S} | = \phi$
\begin{itemize}
\item[] $\mathcal{S} = \mathcal{S} \cup B$. 
\item[] $\mc X = \mc X\setminus B$.
\end{itemize}
\end{itemize}
\end{itemize}
\item[] Output $\mc S := \{B_1',\ldots,B_p'\}$ to the second phase.
%\end{itemize}

\item[] \textbf{Phase 2:}
%\begin{itemize}
\item[] Let $L_C$ denote the list of centers of balls found in the first phase. That is, $L_C = (c_1,\ldots,c_p)$ where $c_i = c(B_i')$.
\item[] if $p < k$
\begin{itemize}
	\item[] No solution exists. Exit. 
\end{itemize}
\item[] Let $\mc L$ denote the list of clusterings found so far. Initialize $\mc L := \phi$
\item[] Let $\mc C_i$ denote the current clustering. Initialize $\mc C_k = \{c_1,\ldots,c_k\}$.
\item[] for $i=k+1$ to $p$
\begin{itemize}
\item[] Let $\mc D$ denote the $(k+1)$-clustering given by $\mc D = \mc C_{i-1} \cup c_i$. 
\item[] Let $c_r, c_s$ ($r < s$) denote the closest two clusters in $\mc D$. That is $$(c_r, c_s) = \argmin_{c_u,c_v \in \mc D, u < v} |c_u -c_v|$$
\item[] $\mc C_i = \mc D \setminus c_s$.
\item[] $\mc L = \mc L \cup \mc C_i$.
%\end{itemize}

%\item[] // We built a forest with $k$ trees corresponding to the $k$ different clusters we found. 
%\item[] Use single linkage to merge the $k$-clusters found so far. Update $T$.
\end{itemize}
\item[] Output $\mc L$ %and $T$.
\label{alg:Known}
\end{alg}
\caption{Alg. for fixed $\min|B_i|$}
\end{algorithm}

Before we prove any results about the algorithm, we need to define some quantities and conventions that will be used in the proof.
\begin{itemize}[nolistsep,noitemsep]
\item $\mc B = \{B_1, \ldots, B_k\}$ - Any set of balls in the dataset $\mc X$ which is $12$-separated, $\nu$-dense and $2$-sparse and $\min |B_i| = t$.
\item $\mc S = \{B_1', \ldots, B_p'\}$ - The set of balls found in the first phase of Alg. \ref{alg:Known}. 
\item $L_C = \{c_1, \ldots, c_p\}$ - The set of centers of the balls in $\mc S$.  
\end{itemize}
Note that we denote the balls that were found by the Alg. \ref{alg:Known} as $B_i'$ and their centers by $c_i$. 
\begin{itemize}[nolistsep,noitemsep]
\item $\mc L = \{\mc C_k, \ldots, \mc C_p\}$ - The set of possible clusterings outputed by Alg. \ref{alg:Known}. 
\end{itemize}

\begin{definition}[Good Index]
\label{defn:goodIdx}
Let $\mc B, \mc L$ and $\mc S$ be as defined above. We say that an index $g$ is good if it has the following properties.
\begin{itemize}[nolistsep,noitemsep]
\item $\forall i \le g$, $\exists m$ such that $B_i' \cap B_m \neq \phi$
%\item $\forall m$, $B_{g+1}' \cap B_m = \phi$
\item $\forall m$, $\exists i \le g$ such that $B_i' \cap B_m \neq \phi$
\end{itemize}
The next lemma shows that there exists a good index.
\end{definition}

\begin{lemma}
Given a $(12,\nu,2)$-nice set $\mc X$. Let $\mc B, \mc L$ and $\mc S$ be as defined above. Define $r:= \max r(B_i)$. Then there exists an index $g$ which is good (Defn. \ref{defn:goodIdx}). In addition, for all $i\le g$, $r(B_i') \le 2r$.
\label{lemma:goodIdx}
\end{lemma}

\begin{proof}
Let $\mc T = \{i: r(B_i') \le 2r\}$. Let $g = \max(\mc T)$. We will claim that $g$ is the required good index. We will prove that it has both the properties by contradiction. 

Assume that $\exists l \le g$ such that for all $m$, $B_l' \cap B_m = \phi$. Now, $r(B_l') \le 2r$. This contradicts the $2$-sparseness of $\mc B$. This proves the first requirement.

Now, assume that $\exists l$ such that for all $i \le g$, $B_i' \cap B_l = \phi$. Let $B$ be some ball with center $x \in B_l$ and radius $r_B \le \max_{y \in B_l} |x-y| \le 2r(B_l)$. Then $B$ contains atleast $t$ points. Also, note that $|c(B)-c(B_i')| > 12r - r -2r = 8r$. Hence, $B \cap \{\cup_{i=1}^g B_i\} = \phi$. Hence, $B$ can be added to $\mc S$ by the algorithm \ref{alg:Known}. This contradicts the maximality of $\mc T$.
\end{proof}

\begin{corollary}
Given a $(12,\nu,2)$-nice set $\mc X$. Let $\mc B, \mc S, L_C$ and $\mc L$ be as defined above. Denote by $r := \max r(B_i)$. Let $g$ be as defined in Lemma \ref{lemma:goodIdx}. Then for all $i,j \le g$
\begin{itemize}[nolistsep,noitemsep]
\item If $B_i' \cap B_m \neq \phi$ and $B_j' \cap B_m\neq\phi$ for some $m$, then $|c_i-c_j| \le 6 r(B_m) \le 6r$.
\item If $B_i' \cap B_m \neq \phi$ and $B_j' \cap B_n\neq\phi$ for some $m,n$, then $|c_i-c_j| > 6r$.
\end{itemize}
\label{cor:centerDist}
\end{corollary}

\begin{proof}
We will use Lemma \ref{lemma:goodIdx} and the  triangle equality to prove the two results. For the first case, $|c_i-c_j| \le |c_i-c(B_m)| + |c(B_m)-c_j| \le r(B_i') + r(B_m) + r(B_m) + r(B_j') \le 2r + r + r + 2r = 6r$. For the second case, $|c_i-c_j| \ge  |c(B_m)-c_j| - |c_i-c(B_m)| \ge |c(B_m)-c(B_n)| - |c(B_n) - c_j| - |c_i-c(B_m)| > 12r - 3r -3r = 6r$.
\end{proof}

\begin{theorem}
\label{thm:givenT}
Given a $(12,\nu,2)$-nice set $\mc X$. Let $\mc B = \{B_1,\ldots,B_k\}$ be any set which is $12$-separated, $\nu$-dense and $2$-sparse and such that $\min |B_i| = t$. Then Algorithm \ref{alg:Known} outputs a list $\mc L$ of possible clusterings of size $|\mc L| \in O(|\mc X|/t)$. $\mc L$ contains a clustering $\mc C$ such that $\mc C|_{\mc B} = \{B_1,\ldots,B_k\}$.
\end{theorem}

\begin{proof}
$\mc L$ contains a list of clusterings $\mc C_k,\ldots,\mc C_p$. Let $g$ be as stated in Lemma \ref{lemma:goodIdx}. Let $\mc C_g$ be the corresponding clustering. We will show that the clustering $\mc C_g$ is such that $\mc C_g|_{\mc B} = \{B_1,\ldots,B_k\}$. 

Let $\mc C_g = \{c_{g(1)},\ldots,c_{g(k)}\}$ and let $B_{g(1)}',\ldots,B_{g(k)}'$ be the corresponding balls. The claim is that for every $m \le k$ there exists $i \le k$ such that $B_m \cap B_{g(i)}' \neq \phi$. For the sake of contradiction, assume that this is not the case. 

Hence, there exists ball $B_n$ (say) such that for all $i$, $B_{g(i)}' \cap B_n = \phi$ for all $i$. From the definition of $g$, we know that for all $i$, $B_{g(i)}$ intersects some ball $B_m$. Hence, from the pigeonhole principle we get that there exists indices $m, i$ and $j$ such that $B_{g(i)} \cap B_m \neq \phi$ and $B_{g(j)} \cap B_m \neq \phi$. From Cor. \ref{cor:centerDist}, we get that $|c_{g(i)}-c_{g(j)}| \le 6r$.

Fromt the definition of $g$, there exists some index $n_1$ such that $B_{n_1}' \cap B_n \neq \phi$. Let $n_1, \ldots, n_r$ be all the indices such that $B_{n_i}' \cap B_n \neq \phi$. Observe that by the $g^{th}$ iteration of ALgorithm \ref{alg:Known}, all the centers $c_{n_1}, \ldots, c_{n_r}$ were deleted. Let $c_{n_r}$ be the last center out of these that was deleted. Then for some $c_i$, $|c_i - c_{n_r}|$ was the minimum distance between any two clusters at some step (say $q^{th}$ step) of the algorithm. Note that $B_i' \cap B_n \neq \phi$. Hence, using Cor. \ref{cor:centerDist} we get that $|c_i - c_{n_r}| > 6r$. 

Observe that centers $|c_{g(i)}-c_{g(j)}| \le 6r < |c_i - c_{n_r}|$. However, $c_{g(i)}$ and $c_{g(j)}$ were not picked at the $q^{th}$ step of the algorithm. This is a contradiction. Hence, we get that $\forall m$, $\exists i$ such that $B_m \cap B_{g(i)} \neq \phi$. Hence, we get that the sets $\mc B$ and $\mc B' = \{B'_{g(1)}, \ldots, B'_{g(k)}\}$ are unique under intersection.

Let $x \in B_i$. WLOG let $B_{g(i)}'$ intersect $B_i$. Now using Lemma \ref{lemma:goodIdx} and the triangle inequality,
%\vspace{-3mm}
\begin{align*}
&|x-c(B_{g(i)}')| \le |x-c(B_i)| + |c(B_{g(i)}')-c(B_i)| \\
&\le |x-c(B_i)| + r(B_{g(i)}')+ r(B_i) \le r+ 2r + r = 4r.
\end{align*} 
For $j \neq i$, using Lemma \ref{lemma:goodIdx} again we get that
%\vspace{-3mm}
\begin{align*} 
&|x-c(B_{g(j)}')| \ge |c(B_{g(j)}')-c(B_i)| - |x-c(B_i)|\\
& \ge |c(B_i)-c(B_j)| - |c(B_{g(j)}')-c(B_j)| - |x-c(B_i)| \\
&> 12r - r -3r = 9r.
\end{align*} 
Hence, we see that the clustering $\mc C_g$ assigns $x$ to the $i^{th}$ cluster. Hence, $\mc C_g|_\mc B = \{B_1,\ldots,B_k\}$.
\end{proof}

\subsection{List of solutions for any $\mc B$}

\begin{algorithm}
\begin{alg}
\item[] Input : $\mc X$, $k$ and parameters $\lambda$, $\nu$ and $\eta$.
\item[] Initialize $\mc L = \phi$. 
\item[] for $t = \nu|\mc X|$ to $\frac{|\mc X|}{k}$ 
\begin{itemize}
\item[] Run Alg. \ref{alg:Known} with $\min |B_i| = t$.
\item[] Let $\mc O$ be the output of the alg \ref{alg:Known}.
\item[] if $\mc O \not\in \mc L$
\begin{itemize}
\item[] $\mc L = \mc L \cup \mc O$.
\end{itemize}
\end{itemize}
Output $\mc L$.
\label{alg:allB}
\end{alg}
\caption{Alg. for any $\mc B$}
\end{algorithm}

\begin{theorem}
\label{thm:allB}
Given a $(12,\nu,2)$-nice set $\mc X$ and $k$. Let $\mc B = \{B_1,\ldots,B_k\}$ be any set which is $12$-separated, $\nu$-dense and $2$-sparse. Then Algorithm \ref{alg:allB} outputs a list $\mc L$ of possible clusterings such that
\begin{itemize}[nolistsep, noitemsep]
\item $|\mc L| \in O(|\mc X|\log (1/k\nu))$. 
\item $\mc L$ contains a clustering $\mc C$ such that $\mc C|_{\mc B} = \{B_1,\ldots,B_k\}$.
\end{itemize}
\end{theorem}

\begin{proof}
Let $\mc B = \{B_1,\ldots,B_k\}$ be any set which is $12$-separated, $\nu$-dense and $2$-sparse. Let $t = \min |B_i|$. We know that $\nu |\mc X| \le t \le |\mc X|/k$. Let $\mc O$ be the list of clusterings outputed when we run Alg. \ref{alg:Known} with $|\min B_i| = t$. We know that there exists $\mc C \in \mc O$ such that $\mc C$ is consistent with $\mc B$. Hence, $\mc C \in \mc L$.

Let $t_1 = \nu |\mc X|$ and $t_2 = \frac{|\mc X|}{k}$. Then, using Thm. \ref{thm:givenT}, we see that 
\begin{align*}
	|\mc L| \le |\mc X|\Big(\frac{1}{t_1}+\ldots+\frac{1}{t_2}\Big) \in O\bigg(|\mc X|\log\frac{1}{k\nu}\bigg)
\end{align*}
\end{proof}

\section{Noise robustness}
\begin{fact}
Given a $(\lambda, \nu, \eta)$-nice set $\mc X$ and $k$. It is easy to see that the amount of noise in $\mc X$ is upper bounded by $(1-k\nu)|\mc X|$. 
\end{fact}

Observe that smaller the $\nu$ larger is the noise allowed in the set $|\mc X|$. However, Thm. \ref{thm:allB} shows that smaller the $\nu$, the larger is the size of the list that we output. Hence, we pay for the noise by allowing ourselves to output a list of larger size.

In this section, we wish to claim that our algorithms can handle noise which is linear in the size of the dataset $|\mc X|$. Recall tha, Alg. \ref{alg:NotKnown} needed that $|\mc X|$ be $(8, \nu, 2)$-nice while Algs. \ref{alg:Known} and \ref{alg:allB} required that the dataset be $(12, \nu, 2)$-nice. 

We will now construct a few simple examples of sets which are $(8, \nu, 2)$-nice and $(12, \nu, 2)$-nice and show that for each of them the noise is linear in $|\mc X|$. For each of them, we will also calculate the length of the list outputed by Alg. \ref{alg:allB}. Also, we will mostly 

\begin{example}

\end{example}

\section{Conclusion and future work}
We introduced a new notion of what it means for a dataset to be {\it clusterable} (Defn. \ref{defn:niceness}). Informally, our notion says that a dataset is clusterable if the following conditions hold. Firstly, it can be covered by a well-separated set of $k$ dense balls. Secondly, the noise is sparse. We then examine conditions under which a unique solution exists (Section \ref{section:uniqueR}) and when multiple solutions are possible (Section \ref{section:structureT}). When a unique solution exists, we give an algorithm that finds the solution (Alg. \ref{alg:NotKnown}). In other case, we show that the solution-space has some structure (Lemma \ref{lemma:structureT}) and then give an algorithm which finds those soluitions (Alg. \ref{alg:Known}). Finally, we give an algorithm (Alg. \ref{alg:allB}) which outputs a list containing all the nice solutions. We prove that our algorithms can handle noise which is linear in the size of the dataset. Previous known noise-robust algorithms could only handle a small amount of noise(say $\epsilon$ fraction of the dataset).

In this work, we decomposed our dataset into nice and noise parts. We required that the nice part of the data be covered by separated and dense balls. In future, we want try other formalizations for the nice part of the data. For example, we can require that the nice part of the data has $\alpha$-centre proximity or it has $\nu$-strict separation. We would then like to propose efficient algorithms for these formalizations under sparse noise.
\bibliographystyle{alpha}
\bibliography{Clusterability} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
