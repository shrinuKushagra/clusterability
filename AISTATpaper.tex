\documentclass[twoside]{article}
\usepackage{aistats2016}
%\usepackage[paper]{nickstyle}

\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb, amsmath, amsfonts, amsthm}
\usepackage{thmtools}
\usepackage{algorithm} 
\usepackage{enumitem}

\newcommand{\mc}{\mathcal}
\setlength{\parindent}{24pt}
\renewcommand{\bar}[1]{\overline{#1}} 

\newtheorem{theorem}{Theorem}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\ConjName}[1]{\label{con:#1}}
\newcommand{\Conj}[1]{Conjecture~\ref{con:#1}}
\newcommand{\ProblemName}[1]{\label{prob:#1}}
\newcommand{\Problem}[1]{Problem~\ref{prob:#1}}
\renewcommand{\dot}{\bullet}
\newcommand{\Tr}{\operatorname{tr}}
\newcommand{\eps}{\epsilon}

\newcommand{\lmax}{\lambda_\mathrm{max}}
\newcommand{\lmin}{\lambda_\mathrm{min}}
\newcommand{\ufinal}{u_\mathrm{final}}
\newcommand{\lfinal}{l_\mathrm{final}}
\newcommand{\umax}{u_\mathrm{max}}

\newcommand{\Symraw}{\mathbb{S}}
\newcommand{\Sym}[1][]{\Symraw^{\ifthenelse{\equal{#1}{}}{m}{#1}}}
\newcommand{\Psd}[1][]{\Symraw_+^{\ifthenelse{\equal{#1}{}}{m}{#1}}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iprod}[2]{\langle #1, #2 \rangle}
\newcommand{\paren}[2][]{#1({#2}#1)}
\newcommand{\qform}[2]{\transp{#2}#1#2}
\newcommand{\transp}[1]{#1^T}



% Simple (outer) environment for algorithms
\newenvironment{outer_alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
        \setlength{\leftmargin}{5pt}
    }
}
{
    \end{list}
}

% Simple environments for algorithms
\newenvironment{alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
    }
}
{
    \end{list}
}

\DeclareMathOperator{\argmin}{argmin}

%%%%% Title %%%%%
\title{\LARGE Clusterability with structural noise}
\author{}


%%%%% Document Body %%%%%
\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Clustering is the process of partitioning a given data set such that objects in the same partition are similar to one another while objects in different partitions are different. The most common clustering methods are often either linkage-based or center-based. In linkage based clustering, the goal is not to find a single partition but a hiearchy of partitions represented using a clustering tree (hierarchical clustering). These methods start by having each point in its own cluster. Then, two closest (or most similar) clusters are merged together using some similarity measure until the whole data is just a single cluster. One of the common drawbacks of these methods is that they are not robust to noise \cite{narasimhan2005q}. 

Center-based clustering methods work with an optimization function. The objective is to find the set of centers which minimize this function. It is known that most of commonly-used objective functions are NP-Hard to optimize. Still, center-based clustering algorithms are routinely and successfully employed in practice.

Hence, to improve the theoretical understanding of clustering and to explain the apparent gap between theory and practice, various notions of clustering have been proposed. The intuition is that real-world data is not `wort-case' and often enjoy some nice properties which are exploited by the clustering algorithms. In this regard, the \emph{CDNM} hypothesis has become quite popular. It says that ``Clustering is difficult only when it does not matter" \cite{daniely2012clustering}.    

Each of the different notions of clustering that have been proposed, attempt to capture some nice property of the data. The results generally follow one of the two directions. One category of results show that under the proposed clusterability notion some known center-based algorithm runs in polynomial time. The other category of results propose some new clustering algorithm and show that the proposed algorithm runs in polynomial time if the data satisfies the niceness conditions. Now, we will survey each of the known clusterability notions and the known results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Outline}
This paper is organized as follows. In section \ref{sec:previous} we will talk about related work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Previous Work}
\label{sec:previous}

Various notions of clusterability has been proposed in the past. The majority of work focus on the fact that clustering can be done efficiently on the data sets that follow some nice structure i.e. they are well clusterable.

Bilu et al \cite{bilu2012stable} consider an input of an optimization problem to be \emph{stable} if small multiplicative perturbations of the input does not change the optimal solution of optimization task. Specially, they give an efficient algorithm to find the max-cut of well stable graphs.
Another line of work \cite{awasthi2012center, balcan2012clustering} , consider similar condition on the input of center-based clustering tasks, and proposed efficient clustering algorithms for sufficiently stable inputs.

Ackerman et. al \cite{ackerman2009clusterability} proposed a notion of clusterability based on the fact that the cost of optimal clustering of a data set should not change considerably if the centers of optimal clusters are slightly perturbed. Given this condition, they design an efficient clustering algorithms that outputs a clustering with near-optimal cost. 

The last clusterability assumption that we want to mention is called {$\alpha$-Center Proximity\cite{awasthi2012center}} and is similar in nature to the condition that we suggest. The definition basically says that a point is $\alpha$-times closer to its own center than to any other center. 

%
%{$(\epsilon, \delta)$-Center Pertubation }
%\cite{ackerman2009clusterability}
%\begin{definition}
%Given a data set $\mc X$. We say that two clusterings $\mc C$ and $\mc C'$ are $\epsilon$-close, if there exists centers $c_1,\ldots, c_k \in \mc C$ and centers $c_1',\ldots, c_k' \in \mc C'$ such that $\|c_i - c_i'\| \le \epsilon$. $\mc X$ is $(\epsilon, \delta)$-CP clusterable, if for every clustering $\mc C$ of $\mc X$ that is $\epsilon$-close to some optimal $k$-clustering of $\mc X$, $Cost(\mc C) \le (1 + \delta) \thinspace Cost(OPT(\mc X))$. 
%\end{definition}
%Ben-David et. al \cite{ackerman2009clusterability} give an algorithm and prove that if $\mc X$ is $(\epsilon, \delta)$-CP clusterable then the algorithm runs in $O(kn^{k/\epsilon^2})$ and outputs a clustering $\mc C$ such that the cost of $Cost(\mc C) \le (1+\delta)\thinspace Cost(OPT(\mc X))$. The authors also refer to this notion as \textbf{Additive Pertubation Robustness (APR)} \cite{ben2015computational}.  
%
%\subsection*{$\alpha$-Pertubation Resilience \cite{bilu2012stable}}
%\begin{definition}
%Given $(\mc X, d)$ where $\mc X$ is the data set and $d$ is a distance function. We say that $d'$ is an $\alpha$-pertubation of $d$ if for every $x,y \in \mc X$, $d(x,y) \le d'(x,y) \le \alpha d(x,y)$. $\mc X$ has $\alpha$-PR if there exists an optimal clustering $\mc C$ for $(\mc X, d)$ such that for every $d'$ which is an $\alpha$-pertubation of $d$, $\mc C$ is also an optimal clustering for $(\mc X, d')$.
%\end{definition}
%The definition basically says that an optimal clustering remains optimal for small multiplicative pertubation of the input. Bilu and Linial \cite{bilu2012stable} consider the problem of max-cut clustering of a graph. They give an algorithm for this problem and prove that it runs in polynomial-time provided $\alpha > \min \{\sqrt{n\Delta}, n/2\}$ where $\Delta$ is the maximum degree of the graph.
%
%Awasthi et. al \cite{awasthi2012center} consider the problem of clustering with center-based objective functions. They give an algorithm based on single-linkage heurestic and prove that it runs in polynomial time if $\alpha \ge 3$. Balcan et. al \cite{balcan2012clustering} make progress on the same problem and give a polynomial-time algorithm for $\alpha \ge 1+\sqrt{2}$. 

%\subsubsection*{$\alpha$-Center Proximity \cite{awasthi2012center}}
%\begin{definition}
%Given $(\mc X, d)$. Let $\mc C = \{c_1, \ldots, c_k\}$ be a clustering of $\mc X$. We say that $\mc X$ has $\alpha$-center proximity w.r.t $\mc C$, if for all $i \le k$ and for all $x \in C_i$, $d(x,c_i) < \alpha d(x, c_j)$ where $j \neq i$.   
%\end{definition}
%The definition basically says that a point is $\alpha$-times closer to its own center than to any other center.  Awasthi et. al \cite{awasthi2012center} showed that center proximity is a weaker notion than pertubation resilience. That is, if an instance is $\alpha$-pertubation resilient then it is also has $\alpha$-center proximity. Both \cite{awasthi2012center} and \cite{balcan2012clustering} essentially work with $\alpha$-center proximity. The former shows gives a polynomial-time algorithm for $\alpha \ge 3$ and then uses this result to show that this algorithm is also pertubation resilient. The latter does the same for $\alpha \ge 1 + \sqrt{2}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Clusterability with noise}
In the previous section, we discussed various notions of clusterability. However, these notions all assume that the entire data set has a particular property. This can be unrealistic in real-life situations. A more realistic scenario would be the one in which most of the data set have a given nice property. The fraction of points which violate the nice property (for eg. $\alpha$-pertubation resilience) is also referred to as noise. We discuss notions of clusterability which take noise into account. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{$\nu$-Separated Balls \cite{ben2014clustering}}
\begin{definition}
Given $\mc X$. Let $\mc I \subseteq \mc X$ be such that $\mc I = \cup_{i=1}^k B_i$ where each $B_i$ is a ball such that rad$(B_i) \le r$ and for all $i \neq j$, $|c(B_i)-c(B_j)| > \nu$ where $c(B)$ denotes the center of a ball.
\end{definition}
The remaining part of the data $\mc N = \mc X \setminus \mc I$ denotes the noise. Ben-David et. al \cite{ben2014clustering} give a generic algorithm for how to augment any center-based algorithm into one which is robust to noise. For small noise, the robustified algorithm outputs a clustering $\mc C'$ of $\mc X$ such that $\mc C'|_{\mc I}$ is the optimal clustering of $\mc I$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{$(\alpha, \epsilon)$-Pertubation Resilience \cite{balcan2012clustering}}
\begin{definition}
Given $(\mc X, d)$. Let $\mc C$ be an optimal clustering for $(\mc X, d)$ under some objective function $\phi$. Let $d'$ be an $\alpha$-pertubation of $d$ and $\mc C'$ be an optimal clustering for $(\mc X, d')$. $\mc X$ has $(\alpha, \epsilon)$-PR if for every $d'$ which is an $\alpha$-pertubation of $d$, $\mc C'$ is $\epsilon$-close to $\mc C$.
\end{definition}
$(\alpha, \epsilon)$-pertubation resilience is a natural relxation of $\alpha$-pertubation resilience. Balcan et. al \cite{balcan2012clustering} obtained positive results for $(\alpha, \epsilon)$-PR when $\phi$ is the $k$-median objective function. They give a polynomial-time algorithm which produces a $O(1+\epsilon/\rho)$ ($\rho$ is fraction of points in the smallest cluster) approximation to the optimum provided $\alpha > 2 + \sqrt{7}$. The number of noisy points that they can handle for the $k$-median objective is $\le n\epsilon$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{$\nu$-Strict Separation \cite{balcan2008discriminative}}
\begin{definition}
Given $(\mc X, d)$. Let $C(x)$ denote the cluster to which $x$ belongs. We say that $\mc X$ has strict separation if for all $y \in C(x)$ and for all $z \not\in C(x)$, $d(x,y) < d(x,z)$. We say that $\mc X$ has $\nu$-strict separation if there exists $\mc X' \subseteq \mc X$ such that $|\mc X\setminus \mc X'| < \nu|\mc X|$ and $\mc X'$ has strict separation.
\end{definition}
Balcan et. al \cite{balcan2008discriminative} show that if size of the smallest cluster is greater than $5\nu|\mc X|$ then their algorithm produces a hierarchical clustering tree such that a pruning of the tree is $\nu$-close to the ground truth clustering. 

\begin{table}
\centering
\caption{Different notions of clusterability. The type and amount of noise these notions can handle.}
\label{table:clusterabilitynotions}
\vspace{2mm}
\begin{tabular}{c|ccc|c}
\hline
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{}\\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Clusterability} \\ \textbf{Notion}\end{tabular}} &  & \textbf{Small noise} &  & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\textbf{Structural}\\ \textbf{noise}\end{tabular}} \\ 
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{}\\\hline
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{}\\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$\nu$-separated balls\\ ($r$ known)\end{tabular}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}$O\big(\frac{1}{k+1}\big)$ \\ \cite{ben2014clustering} \end{tabular}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{Alg. \ref{alg:NotKnown}} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} \\ 
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$\nu$-separated balls\\ ($r$ unknown)\end{tabular}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$??$} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{Alg. \ref{alg:Known}} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} \\
\multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}$(\alpha, \epsilon)$-pertubation \\ resilience\end{tabular}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}$O(\epsilon)$\\ \cite{balcan2012clustering}\end{tabular}} & \multicolumn{1}{c|}{} & \emph{Current work} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{1}{l}{} \\ 
\multicolumn{1}{c|}{$\nu$-strict separation} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}} $O(\nu)$ \\ \cite{balcan2008discriminative}\end{tabular}} & &\emph{Future work} \\ \hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our Contribution}
Table \ref{table:clusterabilitynotions} shows the different notions of clusterability that have been proposed in the literature. All the notions only deal with quantitative noise. That is, they give efficient algorithms when the noise is small. For example, Balcan et. al \cite{balcan2012clustering} give an algorithm for $(\alpha, \epsilon)$-pertubation resilience. Their algorithm can only handle noise which is in $O(\epsilon)$. Similarly, other notions of clusterability can only handle a small amount of noise.

One of the reasons for this is that these algorithms make no additional assumption on noise other than that it should be small. However, it might make more sense to make some structural assumptions on the noise rather than its size. Intuitively, we expect that the noise is \emph{sparse}. That is, it is not `concentrated' at one place and rather spread across the given data set. In other words, we want to say that the noisy points do not pretend to be a cluster. 

In this work, we want to formalize this assumption as \emph{structural sparse noise}. We define our notion of $\nu$-separated balls with sparse noise. Under this assumption, we give two algorithms. The first algorithm deals with the case when the maximum radius of the balls is known to the algorithm. Ben David et. al \cite{ben2014clustering} work with the same assumption. The other algorithm which is slightly more involved works with the case when the maximum radius is not known. In this case, we output a small list of possible solutions such that one of the clusterings in the list is same (on the non-noisy part of data) as the target clustering. 


\section{Problem Statement}

\subsection{Notation}
For any set $B\subset \mc X$, we denote $c(B)$ as the center of $B$ which is defined as the average of points in $B$. Radius of the set $B$ is defined as $r(B)=\max_{x\in B} |x-c(B)|$. 

A $k$-clustering of the set $\mc X$ is defined as the set $\mc C = \{C_1,\ldots,C_k\}$ such that $C_i \subseteq \mc X$, $\cup C_i = \mc X$ and $C_i \cap C_j = \phi$. For any $x \in \mc X$, if $x \in C_i$, we say that $x$ lies in the $i^{th}$ cluster. Sometimes, we also denote the clustering by $\mc C = \{c_1,\ldots,c_k\}$ where $c_i \in \mc X$. For any $x \in \mc X$, $x$ lies in the cluster given by $\argmin |x-c_i|$.

\begin{definition}[Niceness assumption]
% Adding C_{k+1} as garbage collector?
Given a set $\mc X$ and $k$, we say that $\mc X$ is $(\lambda,\nu, \eta)$-nice if there exists balls $\mc B=\{B_1,\ldots,B_k\}\subset \mathcal{X}$ with the following properties.
\vspace{-3mm}
\begin{itemize}
\item{\bf{$\lambda$-separated and $\nu$-dense balls}:}
	\begin{itemize}[nolistsep,noitemsep]
	\item[$\diamond$] For all $i\neq j, |c(B_i)-c(B_j)| > \lambda\max_i \thinspace r(B_i)$
	\item[$\diamond$] $\min_i|B_i| \ge \nu |\mc X|$
	\end{itemize}
\item{\bf{$\eta$-sparse noise}}: For any ball $B$ such that $c(B)\in \mathcal{X}$, if $r(B)\leq \eta \thinspace\max_i  r(B_i)$, then 
%\vspace{-2mm}
\begin{align*}
|B\cap \{X \backslash \cup_i B_i\}| < \min_i |B_i|
\end{align*}
\end{itemize}
\label{defn:niceness}
\end{definition}

\vspace{1mm}
\begin{lemma}
\label{lemma:chknice}
Given a $(\lambda,\nu, \eta)$-nice set $\mc X$, $k$ and a set of balls $\mc B = \{B_1,\ldots,B_k\}$, we can verify if the set $\mc B$ is $\lambda$-separated, $\nu$-dense and $\eta$-sparse in time poynomial in $|\mc X|$ and $k$.
\end{lemma}
\begin{proof}
Checking if the balls are separated and dense takes $O(k^2 + |\mc X|)$. Verifying the sparseness condition takes time $O(|\mc X|^2)$.
\end{proof}

\section{Objective}
Given a $(\lambda, \nu, \eta)$-nice set $\mc X$ and $k$. Let $\mc B = \{B_1,\ldots,B_k\}$ be a set of balls which is $\lambda$-separated, $\nu$-dense and $\eta$-sparse. We say that a $k$-clustering $\mc C' =\{C_1',\ldots,C_k'\}$ of $\mc X$ is consistent with $\mc B$ if
%\vspace{-3mm}
\begin{align*}
	\mc C'|_\mc B = \{B_1,\ldots,B_k\}
\end{align*}
We consider the following different goals for a clustering algorithm. 
\begin{itemize}[noitemsep,nolistsep]
\item {\em Consistent with one} - The algorithm outputs a clustering $\mc C'$ which is consistent with some $\mc B$.
\item {\em List of solutions} - The algorithm outputs a list of clusterings $\mc L$ with the following property. For every $\mc B$, there exists $\mc C' \in \mc L$ that is consistent with $\mc B$.
\item {\em Unique solution} - The algorithm outputs a clustering $\mc C'$ which is consistent with all $\mc B$.
\end{itemize}

The goal of unique solution is very ambitious. In general, a unique solution doesn't always exist. In this section, we wish to examine conditions under which the solution is unique. If the solution is not unique we wish to output a {\em list of solutions}.  

Given any set $\mc B = \{B_1,\ldots, B_k\}$ which is $\lambda$-separated, $\nu$-dense and $\eta$-sparse. We consider two possible parametrizations of the set $\mc B$. First is the maximum radius $r = \max_i r(B_i)$. The second is the number of points in the smallest ball $t = \min_i |B_i|$.

\subsection{Unique solution for fixed $r = \max \thinspace r(B_i)$}
\begin{lemma}
\label{lemma:uniqueR}
Let $\mc X$ be $(\lambda, \nu, \eta)$-nice. Let $\mc B' = \{B_1',\ldots, B_k'\} \subseteq \mc X$ be any set with $r = \max_i r(B_i)$ which is $\lambda$-separated, $\nu$-dense and $\eta$-sparse. Then there exists a set $\mc B(r) = \{B_1,\ldots,B_k\}$ such that the sets $\mc B(r)$ and $\mc B'$ are unique under intersection. That is 
\begin{itemize}[nolistsep]
\item For all $i$, $B_i \cap B_i' \neq \phi$. 
\item For all $i\neq j$, $B_i \cap B_j' = \phi$.
\end{itemize}
\end{lemma} 
\begin{proof}
\end{proof}

\begin{theorem}
\label{theorem:uniqueR}
Let $\mc X, \mc B', \mc B(r)$ be as defined in Lemma \ref{lemma:uniqueR}. Let $\mc C(r)$ be the clustering induced by the centers of $\mc B(r)$. Then, $\mc C(r)|_{B'} = \{B_1', \ldots, B_k'\}$. 
\end{theorem}
\begin{proof}
\end{proof}

Thm. \ref{theorem:uniqueR} shows that for a fixed value of $r$, $\mc C(r)$ is the unique solution for all $\mc B'$ such that $\max_i r(B_i') = r$. The proof of Lemma \ref{lemma:uniqueR} actually gives an algorithm   

\begin{algorithm}
\begin{alg}
\item[] Set $r = \max_i \thinspace r(B_i)$
\item[] for $i=1$ to $k$
	\begin{itemize}
		\item[] Let $B_i'$ be the densest ball of radius $2r$ such that $c(B_i') \in \mathcal{X}$.
		\item[] Set $\mc X=\mc X\setminus B_{4r}(c(B_i'))$. 
	\end{itemize}
\item[] Let $\mc B' := \{B_1',\ldots,B_k'\}$ and $c(\mc B') = \{c(B_1'),\ldots,c(B_k')\}$.
\item[] Output $\mc B'$ and the clustering $\mc C'$ induced by $c(\mc B')$.
\label{alg:NotKnown}
\end{alg}
\caption{Alg. for known $\max{r(B_i)}$}
\end{algorithm}

\begin{theorem}
Given a $(2,8)$-nice set $\mc X$. Let $\mc B = \{B_1,\ldots,B_k\}$ be the set of any balls that satisfy the separation and sparseness condition. Given $r = \max r(B_i)$, then Algorithm \ref{alg:NotKnown} outputs a set $\mc B' = \{B_1',\ldots,B_k'\}$ such that 
\begin{itemize}
	\item The sets $\mc B$ and $\mc B'$ are unique under intersection. That is there exists a permutation of the set $\mc B$ such that, for all $i\neq j$, $B_i \cap B_j' = \phi$ and for all $i$, $B_i \cap B_i' \neq \phi$
	\item The clustering $\mc C'$ induced by $\mc B'$ is such that $\mc C'|_\mc B = \{B_1,\ldots,B_k\}$.	
\end{itemize}

\end{theorem}
\begin{proof} Let $B_1',\ldots,B_i'$ be the balls found till the $i^{th}$ iteration. Also, observe that after every iteration we delete some points from $\mc X$. Let $\mc X_i$ denote the points in $\mc X$ after the $i^{th}$ iteration. 

\textit{Induction hypothesis}: For all $1\le i \le l$, $B_i \cap B_i' \neq \phi$ and $X_l \cap \{\cup_{i=1}^l B_i\} = \phi$.

\textit{Base case}: We wish to prove that $B_1 \cap B_1' \neq \phi$. Assume for the sake of contradiction, that $B_1'$ is such that for all $i$, $B_1' \cap B_i = \phi$. Now, $B_1' \cap \{X \backslash \cup_{i\in[k]} B_i\} = B_1'$. Observe that $B_1'$ is the densest ball of radius $2r$ with $c(B_1') \in \mathcal{X}$. Hence, for all $i$, $|B_1'| \ge |B_i|$. Hence, $|B_1'| \ge \min_{i=1}^k |B_i|$. Thus, $|B_1' \cap \{X \backslash \cup_{i\in[k]} B_i\}| \ge \min |B_i|$ which contradicts the fact that $\mc B$ satisfies the sparseness condition. Thus, $\exists i$ s.t. $B_i \cap B_1' \neq \phi$. Without loss of generality, let $B_i = B_1$. Now, $r(B_1) \le r$, $r(B_1') = 2r$ and $B_1 \cap B_1' \neq \phi$, hence $|c(B_1')-c(B_1)| \le 3r$. Thus, $B_1 \subseteq B_{4r}(c(B_1'))$ and hence $X_1 \cap B_1 = \phi$. 

\textit{Induction step}: We are given that for all $1\le i \le l$, $B_i \cap B_i' \neq \phi$. We wish to prove that $B_{l+1} \cap B_{l+1}' \neq \phi$. The proof for the induction step follows the exact same arguments as the base case.

Hence, we get that for all $i$, $B_i \cap B_i' = \phi$. Separation $\nu = 8$ implies that for all $j\neq i$, $B_i' \cap B_j' = \phi$ and $B_i' \cap B_j = \phi$. This completes the proof of the first part of the theorem.

Let $x \in B_i$. Observe that, $|c(B_i)-c(B_i')| \le 3r$ and for all $j\neq i$, $|c(B_i')-c(B_j)| > 5r$. Now, $|x-c(B_i')| \le |x-c(B_i)| + |c(B_i')-c(B_i)| \le r+3r = 4r$. For $j \neq i$, $|x-c(B_j')| \ge |c(B_j')-c(B_i)| - |x-c(B_i)| > 5r - r = 4r$. Hence, we see that the clustering $\mc C'$ assigns $x$ to the $i^{th}$ cluster. Hence, $\mc C'|_\mc B = \{B_1,\ldots,B_k\}$.
\end{proof}

\begin{theorem}
Algorithm \ref{alg:NotKnown} runs in time $O($poly$(|\mathcal{X}|,k))$.
\end{theorem}
\begin{proof}
During each iteration, we find the densest ball of radius $2r$. For each point in $\mathcal{X}$, we count the number of points whose distance is $\le 2r$. This can take $O(n)$ in the worst case. Hence, finding the densest ball takes $O(n^2)$ time. Similarly, deleting points from $\mathcal{X}$ takes $O(n^2)$ time. Hence, the for-loop takes $O(n^2k)$ time.
\end{proof}

\subsection{Finding a solution for  fixed $t = \min |B_i|$}
%All algoritthms are polynomial in n and k
In this case, we assume that the algorithm doesn't know the value of $r = \max r(B_i)$. We assume that the algorithm knows the number of points in the smallest cluster $\min |B_i| = t$ (say). This algorithm works in two phases.

\begin{algorithm}
\begin{alg}
\item[] \textbf{Phase 1:}
%\begin{itemize}
\item[] Let $\mc S$ denote the balls found so far. Initialize $\mc S = \phi$.
\item[] Let $D$ be the sorted list of distances between all pairs of points in $\mc X$. $D = \{d_1,\ldots,d_t\}$. Note that $t \le |\mc X|^2/2$.
\item[] for $i=1$ to $t$
\begin{itemize}
\item[] foreach $x \in \mc X$
\begin{itemize}
\item[] Let $B$ be a ball of radius $d_i$ centered at $x$.
\item[] if $|B| \ge t$ and $|B \cap \mathcal{S} | = \phi$
\begin{itemize}
\item[] $\mathcal{S} = \mathcal{S} \cup B$. 
\item[] $\mc X = \mc X\setminus B$.
\end{itemize}
\end{itemize}
\end{itemize}
\item[] Output $\mc S := \{B_1',\ldots,B_p'\}$ to the second phase.
%\end{itemize}

\item[] \textbf{Phase 2:}
%\begin{itemize}
\item[] Let $L_C$ denote the list of centers of balls found in the first phase. That is, $L_C = (c_1,\ldots,c_p)$ where $c_i = c(B_i')$.
\item[] Let $\mc L$ denote the list of clusterings found so far. Initialize $\mc L := \phi$
\item[] Let $\mc C_i$ denote the current clustering. Initialize $\mc C_k = \{c_1,\ldots,c_k\}$.
\item[] for $i=k+1$ to $p$
\begin{itemize}
\item[] Let $\mc D$ denote the $(k+1)$-clustering given by $\mc D = \mc C_{i-1} \cup c_i$. 
\item[] Let $c_r, c_s$ ($r < s$) denote the closest two clusters in $\mc D$. That is $$(c_r, c_s) = \argmin_{c_u,c_v \in \mc D, u < v} |c_u -c_v|$$
\item[] $\mc C_i = \mc D \setminus c_s$.
\item[] $\mc L = \mc L \cup \mc C_i$.
%\end{itemize}

%\item[] // We built a forest with $k$ trees corresponding to the $k$ different clusters we found. 
%\item[] Use single linkage to merge the $k$-clusters found so far. Update $T$.
\end{itemize}
\item[] Output $\mc L$ %and $T$.
\label{alg:Known}
\end{alg}
\caption{Alg. for known $\min{B_i}$}
\end{algorithm}

Before we prove any results about the algorithm, we need to define some quantities and conventions that will be used in the proof.
\begin{itemize}
\item $\mc B = \{B_1, \ldots, B_k\}$ - The set of balls in the dataset $\mc X$ that satisfy the separation and sparseness condition (Defn. \ref{defn:niceness})
\item $\mc S = \{B_1', \ldots, B_p'\}$ - The set of balls found in the first phase of Alg. \ref{alg:Known}. 
\item $L_C = \{c_1, \ldots, c_p\}$ - The set of centers of the balls in $\mc S$.  
\end{itemize}
Note that we denote the balls that were found by the Alg. \ref{alg:Known} as $B_i'$ and their centers by $c_i$. 
\begin{itemize}
\item $\mc L = \{\mc C_k, \ldots, \mc C_p\}$ - The set of possible clusterings outputed by Alg. \ref{alg:Known}. 
\end{itemize}

\begin{definition}[Good Index]
\label{defn:goodIdx}
Let $\mc B, \mc L$ and $\mc S$ be as defined above. We say that an index $g$ is good if it has the following properties.
\begin{itemize}
\item $\forall i \le g$, $\exists m$ such that $B_i' \cap B_m \neq \phi$
%\item $\forall m$, $B_{g+1}' \cap B_m = \phi$
\item $\forall m$, $\exists i \le g$ such that $B_i' \cap B_m \neq \phi$
\end{itemize}
The next lemma shows that there exists a good index.
\end{definition}

\begin{lemma}
Given a $(2,12)$-nice set $\mc X$. Let $\mc B, \mc L$ and $\mc S$ be as defined above. Define $r:= \max r(B_i)$. Then there exists an index $g$ which is good (Defn. \ref{defn:goodIdx}).
\label{lemma:goodIdx}
\end{lemma}

\begin{proof}
Let $\mc T = \{i: B_i' \cap B_m = \phi$ for all $m\}$. If $\mc T = \phi$ then let $q = p+1$. Otherwise let $q = \min(\mc T)$. We will claim that $g = q-1$ is the required good index in both cases.

From the definition of the set $\mc T$ and $g$, we know that $\forall i \le g$, $B_i' \cap B_m \neq \phi$ for some $m$. Hence, we see that the index $g$ satisfies the first goodness criteria. We prove that it also satisfies the second criteria by contradiction. Assume that there exists $l$ such that $\forall i \le g$, $B_i' \cap B_l = \phi$.

Let $B$ be some ball with center $x \in B_l$ and radius $r_B \le \max_{y \in B_l} |x-y| \le 2r(B_l)$. Then $B$ contains atleast $t$ points. Observe that since $B_t' \cap B_m = \phi$ for all $m$ and $\mc X$ is $(2,12)$-nice, $r(B_t') > 2r$. Hence, Alg \ref{alg:Known} adds $B$ to $\mc S$ before it adds $B_t'$. Hence, $B = B_i'$ for some $i\le g$. This contradicts the assumption that $B_i' \cap B_l = \phi$ for all $i \le g$. Hence, we get that there exists a good index. 
\end{proof}


\begin{lemma}
Given a $(2,12)$-nice set $\mc X$. Let $\mc B, \mc S$ and $\mc L$ be as defined above. Given $t = \min |B_i|$. Denote by $r := \max r(B_i)$. Let $g = \min \{i: i$ is a good index\}. Then for all $i\le g$, $r(B_i') \le 2r$.

\label{lemma:centerDist}
\end{lemma}

\begin{proof}
Let $\mc T = \{B_1', \ldots, B_g'\}$. Alg. \ref{alg:Known} considers the radius of balls in sorted order. Hence, $r(B_1') \le r(B_2') \le \ldots \le r(B_g')$. Since $g$ is good, there exists $B_l$ such that $B_g' \cap B_l \neq \phi$. The claim is that $B_g'$ is the only ball in $\mc T$ which intersects $B_l$. Assume that this is not the case. Then there exists $h < g$ such that $B_h' \cap B_l \neq \phi$. Hence, the index $g-1$ is good. This contradicts the minimality of $g$. 

Hence, $B_g'$ is the only ball that intersects $B_l$. Now, we claim that $\forall i \le g, r(B_i') \le 2r(B_l)$. Assume that this is not the case. Let $q$ be the first index such that $r(B_q') > 2r(B_l) \ge r(B_{q-1})$. Let $B$ be some ball with center $x \in B_l$ and radius $r_B \le \max_{y \in B_l} |x-y| \le 2r(B_l)$. Then $B$ contains atleast $t$ points. Hence, Alg \ref{alg:Known} adds $B$ to $\mc S$ before it adds $B_q'$. Hence, $B$ is equal to one of $B_1', \ldots, B_{q-1}'$. This contradicts the fact that $\forall i \le g-1$, $B_i' \cap B_l = \phi$. Thus, $r(B_g') \le 2r(B_l) \le 2r$. 
\end{proof}

\begin{corollary}
Given a $(2,12)$-nice set $\mc X$. Let $\mc B, \mc S, L_C$ and $\mc L$ be as defined above. Given $t = \min |B_i|$. Denote by $r := \max r(B_i)$. Let $g = \min \{i: i$ is a good index\}. Then for all $i,j \le g$
\begin{itemize}
\item If $B_i' \cap B_m \neq \phi$ and $B_j' \cap B_m\neq\phi$ for some $m$, then $|c_i-c_j| \le 6 r(B_m) \le 6r$.
\item If $B_i' \cap B_m \neq \phi$ and $B_j' \cap B_n\neq\phi$ for some $m,n$, then $|c_i-c_j| > 6r$.
\end{itemize}
\label{cor:centerDist}
\end{corollary}

\begin{proof}
We will use Lemma \ref{lemma:centerDist} and the  triangle equality to prove the two results. For the first case, $|c_i-c_j| \le |c_i-c(B_m)| + |c(B_m)-c_j| \le r(B_i') + r(B_m) + r(B_m) + r(B_j') \le 2r + r + r + 2r = 6r$. For the second case, $|c_i-c_j| \ge  |c(B_m)-c_j| - |c_i-c(B_m)| \ge |c(B_m)-c(B_n)| - |c(B_n) - c_j| - |c_i-c(B_m)| > 12r - 3r -3r = 6r$.
\end{proof}

\begin{theorem}
Given a $(2,12)$-nice set $\mc X$. Let $\mc B = \{B_1,\ldots,B_k\}$ be the set of any balls that satisfy the separation and sparseness condition. Given $t = \min |B_i|$, Algorithm \ref{alg:Known} outputs a list $\mc L$ of possible clusterings of size $|\mc L| \in O(|\mc X|)$. $\mc L$ contains a clustering $\mc C$ such that $\mc C|_{\mc B} = \{B_1,\ldots,B_k\}$.
\end{theorem}

\begin{proof}
$\mc L$ contains a list of clusterings $\mc C_k,\ldots,\mc C_p$. Let $g$ be the first index which has the properties as stated in Lemma \ref{lemma:goodIdx}. Let $\mc C_g$ be the corresponding clustering. We will show that the clustering $\mc C_g$ is such that $\mc C_g|_{\mc B} = \{B_1,\ldots,B_k\}$. 

Let $\mc C_g = \{c_{g(1)},\ldots,c_{g(k)}\}$ and let $B_{g(1)}',\ldots,B_{g(k)}'$ be the corresponding balls. The claim is that for every $m \le k$ there exists $i \le k$ such that $B_m \cap B_{g(i)}' \neq \phi$. For the sake of contradiction, assume that this is not the case. 

Hence, there exists ball $B_n$ (say) such that for all $i$, $B_{g(i)}' \cap B_n = \phi$ for all $i$. From the definition of $g$, we know that for all $i$, $B_{g(i)}$ intersects some ball $B_m$. Hence, from the pigeonhole principle we get that there exists indices $m, i$ and $j$ such that $B_{g(i)} \cap B_m \neq \phi$ and $B_{g(j)} \cap B_m \neq \phi$. From Cor. \ref{cor:centerDist}, we get that $|c_{g(i)}-c_{g(j)}| \le 6r$.

Fromt the definition of $g$, there exists some index $n_1$ such that $B_{n_1}' \cap B_n \neq \phi$. Let $n_1, \ldots, n_r$ be all the indices such that $B_{n_i}' \cap B_n \neq \phi$. Observe that by the $g^{th}$ iteration of ALgorithm \ref{alg:Known}, all the centers $c_{n_1}, \ldots, c_{n_r}$ were deleted. Let $c_{n_r}$ be the last center out of these that was deleted. Then for some $c_i$, $|c_i - c_{n_r}|$ was the minimum distance between any two clusters at some step (say $q^{th}$ step) of the algorithm. Note that $B_i' \cap B_n \neq \phi$. Hence, using Cor. \ref{cor:centerDist} we get that $|c_i - c_{n_r}| > 6r$. 

Observe that centers $|c_{g(i)}-c_{g(j)}| \le 6r < |c_i - c_{n_r}|$. However, $c_{g(i)}$ and $c_{g(j)}$ were not picked at the $q^{th}$ step of the algorithm. This is a contradiction. Hence, we get that $\forall m$, $\exists i$ such that $B_m \cap B_{g(i)} \neq \phi$. Hence, we get that the sets $\mc B$ and $\mc B' = \{B'_{g(1)}, \ldots, B'_{g(k)}\}$ are unique under intersection.

Let $x \in B_i$. WLOG let $B_{g(i)}'$ intersect $B_i$. Now using Lemma \ref{lemma:centerDist} and the triangle inequality,
%\vspace{-3mm}
\begin{align*}
&|x-c(B_{g(i)}')| \le |x-c(B_i)| + |c(B_{g(i)}')-c(B_i)| \\
&\le |x-c(B_i)| + r(B_{g(i)}')+ r(B_i) \le r+ 2r + r = 4r.
\end{align*} 
For $j \neq i$, using Lemma \ref{lemma:centerDist} again we get that
%\vspace{-3mm}
\begin{align*} 
&|x-c(B_{g(j)}')| \ge |c(B_{g(j)}')-c(B_i)| - |x-c(B_i)|\\
& \ge |c(B_i)-c(B_j)| - |c(B_{g(j)}')-c(B_j)| - |x-c(B_i)| \\
&> 12r - r -3r = 9r.
\end{align*} 
Hence, we see that the clustering $\mc C_g$ assigns $x$ to the $i^{th}$ cluster. Hence, $\mc C_g|_\mc B = \{B_1,\ldots,B_k\}$.
 
\end{proof}



\bibliographystyle{alpha}
\bibliography{Clusterability} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
