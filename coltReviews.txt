----------------------- REVIEW 1 ---------------------
PAPER: 198
TITLE: Finding meaningful cluster structure amidst background noise
AUTHORS: Shai Ben-David, Shrinu Kushagra and Samira Samadi

OVERALL EVALUATION: 1 (weak accept: prefer to accept but will not push for it; top 20%-40%)
REVIEWER'S CONFIDENCE: 4 (high)

----------- Review -----------
This paper extends on the recent work that aims to develop algorithms for clustering instances that satisfy certain assumptions, as a way of avoiding NP-hardness of worst-case instances.  The authors of this paper refer to these assumptions "clusterability assumptions," though they seem to be called various names in the literature.

The most popular of these assumption include "stability" and "center proximity," and algorithms exist for both these settings for sufficiently "nice" instances.  However, one downside of many of these results (with the exception, I think, of the recent work of Balcan and Liang) is that in order for the guarantees to carry through, the instances must satisfy the needed clusterability assumptions exactly.

This work relaxes this constraint and gives algorithms in cases when the assumptions aren't satisfied exactly, but only approximately.  This is seen as "noise," and the algorithms here work under some assumptions on the noise.  The results appear in two main sections, one for each of the main center-based clusterability assumptions.  The noise that is considered is "sparse noise" meaning that the noise is evenly distributed through the instance.  I would guess that random noise would satisfy this noise assumption, though I don't think the relationship between the two is formalized in the paper.  Lower bounds are also given for arbitrary noise cases, though there are some gaps.

The first algorithm (for section) is reminiscent of the Balcan-Liang, and it builds a tree hierarchically; as long as a certain distance condition is satisfied, clusters are hierarchically merged.  The second algorithms (for the second condition) is also rather natural -- a graph is built from intersections of "balls" and the connected components of this graph is the clustering.  Though it seems strange, the relationship to the stability conditions is not so hard to see.  I didn't check the details of all the proofs, but the claims seem believable, and the algorithms are fairly natural.

One the positive side, noiseless graphs don't really exist in the real world, and a serious research effort for this problem really requires handling the "noisy case", and this paper is one of the first general contributions to this line of research.  On the negative side, I think these clusterability assumptions are not so realistic, and the large constants involved in the upper (and lower!) bounds make me worried the wrong problem is being studied.  However, once noise is allowed, these assumptions become possibly more realistic, so this downside doesn't bother me as much in this paper as in other papers in this field.

notes:

- pg. 9 till -> until
- The relationship between the assumptions that you work with and what you call "strong stability" is hidden in the proof of theorem 15.  It should be made more explicit.  One of the problems of the various assumptions is that it is hard to keep track of which results (from which papers) apply to which other models.

----------------------- REVIEW 2 ---------------------
PAPER: 198
TITLE: Finding meaningful cluster structure amidst background noise
AUTHORS: Shai Ben-David, Shrinu Kushagra and Samira Samadi

OVERALL EVALUATION: 1 (weak accept: prefer to accept but will not push for it; top 20%-40%)
REVIEWER'S CONFIDENCE: 5 (expert)

----------- Review -----------
Finding Meaningful Clusters Amidst Background Noise
----------------------------------------------------

The paper continues to add to TCS literature on clustering under stability assumptions. But whereas the majority of this literature assumes the entire instance satisfies some stability assumption (with BL12 being a notable exception), this paper deals with instances that are composed of a subinstance satisfying some stability property, often referred to as the good clustering, and noise --- additional datapoints that are not too dense. The noise concept that the authors deal with is what they refer to as sparse noise: any ball that contains at least half of min{size of good-cluster} many points must have radius that is greater than \eta times max{radius of a good-cluster}. The authors tackle two stability notions: \alpha-center proximity (for any point, the distance to the nearest center is smaller by an \alpha factor than the distance to the second closest center) and \lambda-center separation (the distance between any two center points is at least \lambda times greater tha!
 n max{radius of a good-cluster}). The authors give two positive results for the noisy case: an algorithm (Algorithm 1) that returns a clustering tree laminar with the (or any) good subclustering in the sparse \alpha-center proximity case; and an algorithm (Algorithm 3) that returns a clustering that is laminar with the good subclustering in the sparse \lambda-center separation case. They also deal with the noiseless (like the standard literature) case of \lambda-center separation (Section 4.1); and give a variety of negative results showing that no clustering tree can be laminar with all good sub-instances for certain values of \alpha,\lambda and the sparseness parameter \eta. A more detailed description of the results appears in Section 3's intro (p.6) and Section 4.2's intro (p. 10) in the form of bulletin points.

In addition to my own personal preference to see the subject of clustering under stability assumptions represented in COLT, I also am a great supporter of a high-level message that the paper eloquently describe on page 2: "There is no way a user can know what is the cost of the optimal clustering ... Instead, a user might have a notion of a meaningful cluster structure and will be happy with any outcome that meets such a requirement." In my opinion, that is an important message and in fact can form a basis for a whole line of works. (Though this basis was, to some extant, already suggested by BBV08.)

And so, I do see this paper as a first step in the right direction. However, there are multiple issues that bother me with the paper.
1. I strongly object to the validity of the negative results. Just because the given clusterings aren't laminar with any tree, there could still be a tractable algorithm that outputs a short description of few selected clusterings that are laminar with the ones posed in the counter examples. E.g., some of the algorithms in BBV08's seminal work output a list of possible clusterings. This leaves only Theorem 16 as a true lower bound in this paper...
2. Please move Section 4.1 to the appendix. It basically reiterates known results (Theorem 15 is basically like ABS10, Theorem 16 is basically Reyzin12).
3. The sparseness noise with \eta\geq 1 and center-separation that uses max-radius (rather than the radii of C_i, C_j) are two properties that seem very strong. Are instances satisfying such properties are easy to come by? For example, suppose I add to a stable instance a set of m u.a.r points from a ball / cube containing the instance - is this likely to satisfy the sparsity condition? It would be nice if the authors can point to an instance in which standard algorithms, such as single-, average- or max-linkage, fail but Algorithm 1 does find a good clustering.
4. The presentation of the paper is very much lacking. This includes some minor issues (e.g., sometimes m(C) and r(C) appear without \mathcal{C}, like in p.6&10), but also unnecessarily convoluted descriptions of the algorithms (e.g., Algorithm 3's first phase is simply: let L be the set of all balls $B(p, d(p,q))$ for any p,q satisfying d(p,q)\leq r that contain at least t points.), and insufficient description in places it can help the paper greatly (e.g.: right after Definitions 6&8; key points in the proofs are omitted). Moreover, the authors fail to put this work in the greater picture: no discussion and/or open problems section, no discussion of the existence of a pareto-curve for the two parameters (stability parameter vs. sparseness parameter \eta).
More detailed comments about presentation are below.

In conclusion, I support weak acceptance of the paper. It will contribute to COLT, but rejecting it and letting the authors "clean" it and expand on it can drastically improve the paper.

Concrete comments:
* Cite BBV08 at the intro.
* Isn't related work section repetitive given the intro?
* Before Definition 4, consider defining a clustering tree.
* (My pet peeve) Omit the use of shorthand notation (Defn => Definition, Thm => Theorem, Alg => Algorithm, etc.)
* "arbitrary noise", "sparse noise" and "noiseless" --- define those terms before using them in titles.
* P.5: what does "sitting on the same location" mean?
* Theorem 10: does t(C) mean m(C)?
* Algorithm 3: why not assign remaining points arbitrarily? Doesn't seem to play any role in your analysis...
* P.13: definition of Y_B^C should start with "given a ball B and a clustering C"
* P.14 F.1: "the distance being considered"? is that d(p,q)?
* Claim 21: argue the second part by symmetry (unless I missed something).
* Claim 23: shouldn't |B|=|S^*_i| be \geq? B can include points not from S... Shouldn't |C|-|C\setminus B| = |C| - |B'\setminus S_i^*| for the same reason?
* Proof of Theorem 18: - s_i denote the good clusters' centers? - B=S_i , again can't B contain points not from S_i?
* Claims 25 & 26: the key points are the two sentences that include $\lambda \geq 4$ --- and you need to expand on why the facts stated in these sentences are true.
* Proof of Theorem 19: shouldn't it be O(n^3) (naively)? There are n^2 balls to consider, and counting how many points reside in them takes O(n) per each...

----------------------- REVIEW 3 ---------------------
PAPER: 198
TITLE: Finding meaningful cluster structure amidst background noise
AUTHORS: Shai Ben-David, Shrinu Kushagra and Samira Samadi

OVERALL EVALUATION: 1 (weak accept: prefer to accept but will not push for it; top 20%-40%)
REVIEWER'S CONFIDENCE: 4 (high)

----------- Review -----------
This paper follows the line of work initiated by Balcan, Blum and Vempala (2008) on a PAC style framework for clustering points in a metric space, and provides conditions under which this clustering can be done in the presence of background noise. The main innovation in this paper appears to be how to define “background noise”; this is done through a “sparse noise condition” which essentially states that in any ball of large enough radius (where large enough means a constant factor of the radius of the largest cluster), there are a small number of noise points (where again small number means less than half of the size of the smallest cluster). Provided the clusters themselves are quite well separated, this paper provides a computationally efficient algorithm to construct a tree that contains a pruning that captures the ground truth clustering.

Strengths:

- The problem studied is quite interesting — after all, formalizing a notion of clustering is a long-studied yet challenging open problem in the learning theory community and clustering when there is background noise is a situation that often happens in reality.
- The sparse noise condition is also quite nice, although the paper does not discuss how realistic it is, especially under the parameter values considered in this paper.
- The algorithm in this paper is novel, although the ideas are somewhat similar to those in previous work such as Balcan, Blum and Vempala and Balcan and Liang.

Weaknesses:

- The main weakness in this paper is the lack of justification as to how realistic the assumptions are. Center proximity by itself is not so bad, but alpha being equal to 4.6 perhaps seems a little unrealistic. Of course it is possible that the algorithm is robust to these assumptions and works just fine on real data; it would be good to be confirm this through some experiments.

Overall, this is a nice paper that extends previous work on a PAC style framework of clustering to clustering in the presence of background noise. I think it should be accepted if possible.

