\documentclass[11pt]{article}


%%%%% Packages %%%%%
\usepackage[paper]{nickstyle}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb, amsmath}



%%%%% Macros %%%%%
\setlength{\parindent}{24pt}
\renewcommand{\bar}[1]{\overline{#1}}

\newtheorem{fact}[theorem]{Fact}
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{problem}[theorem]{Problem}
\newcommand{\ConjName}[1]{\label{con:#1}}
\newcommand{\Conj}[1]{Conjecture~\ref{con:#1}}
\newcommand{\ProblemName}[1]{\label{prob:#1}}
\newcommand{\Problem}[1]{Problem~\ref{prob:#1}}
\renewcommand{\dot}{\bullet}
\newcommand{\Tr}{\operatorname{tr}}
\newcommand{\eps}{\epsilon}

\newcommand{\lmax}{\lambda_\mathrm{max}}
\newcommand{\lmin}{\lambda_\mathrm{min}}
\newcommand{\ufinal}{u_\mathrm{final}}
\newcommand{\lfinal}{l_\mathrm{final}}
\newcommand{\umax}{u_\mathrm{max}}

\newcommand{\Symraw}{\mathbb{S}}
\newcommand{\Sym}[1][]{\Symraw^{\ifthenelse{\equal{#1}{}}{m}{#1}}}
\newcommand{\Psd}[1][]{\Symraw_+^{\ifthenelse{\equal{#1}{}}{m}{#1}}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iprod}[2]{\langle #1, #2 \rangle}
\newcommand{\paren}[2][]{#1({#2}#1)}
\newcommand{\qform}[2]{\transp{#2}#1#2}
\newcommand{\transp}[1]{#1^T}



% Simple (outer) environment for algorithms
\newenvironment{outer_alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
        \setlength{\leftmargin}{5pt}
    }
}
{
    \end{list}
}

% Simple environments for algorithms
\newenvironment{alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
    }
}
{
    \end{list}
}




%%%%% Title %%%%%
\title{\LARGE Noise robust clustering}
\author{}


%%%%% Document Body %%%%%
\begin{document}
\maketitle

\section{Introduction}
\begin{itemize}
\item Nika and Shai ICML'14
\end{itemize}

\section{Problem Statement}

\subsection{Notation}
For any set $B\subset X$, we denote $c(B)$ as the center of $B$ which is defined as the average of points in $B$. Radius of the set $B$ is defined as $r(B)=\max_{x\in B} |x-c(B)|$. Define the induced clustering.

\begin{definition}[Niceness assumption]
% Adding C_{k+1} as garbage collector?
Given a set $\mathcal{X}$, we say that $\mathcal{X}$ is $(\lambda,\nu)$-nice if there exists a clustering of $\mathcal{X}$, $P=\{P_1,\ldots,P_k\}$ which satisfies the following conditions. There exists balls $B=\{B_1,\ldots,B_k\}\subset \mathcal{X}$ such that for every $i\in[k]$, there exists $j_i\in[k]$ such that $B_i\subset P_{j_i}$ and
\begin{itemize}
\item{\bf{Separation}:} For all $i,j\in[k], |c(B_i)-c(B_j)| > \nu\cdot\max_t \thinspace r(B_t)$
\item{\bf{Sparse Noise}}: For any ball $B$ such that $c(B)\in \mathcal{X}$, if $r(B)\leq \lambda \cdot \max \thinspace r(B_i)$, then $|B\cap \{X \backslash \cup_{i\in[k]} B_i\}| < \min_i |B_i|$.
\end{itemize}
\end{definition}

\noindent {\bf Goal}: Given the set $\mathcal{X}$ and the value $k$, our goal is to design an algorithm that does $k$-clustering $C=\{C_1,\ldots,C_k\}$ on set $\mathcal{X}$ such that  for any $(\lambda,\nu)$-nice clustering $P$ with sets $B = \{B_1,\ldots,B_k\}$ the induced clustering of $C$ on $B$, i.e., $C|_B = \{B_1,\ldots,B_k\}$. 

\begin{lemma}
\label{lemma:chknice}
Given a $(\lambda,\nu)$-nice set $\mathcal{X}$. Given a $k$-clustering $C = \{C_1,\ldots,C_k\}$ of $\mathcal{X}$ and corresponding balls $B = \{B_1,\ldots,B_k\}$, i.e. $B_i \subseteq C_i$, we can check if $C$ is nice in time poynomial in $|\mathcal{X}|$ and $k$.
\end{lemma}
\begin{proof}
We can check if the balls $B_i$ satisfy the separation condition in time $O(k^2)$. To check if they satisfy the sparse noise condition, it is easy to see that it takes time $O(n^2)$ where $n = |\mathcal{X}|$. Hence, given the set of balls and the clustering induced by the balls $(C,B)$, we can test in $O(n^2+k^2)$ time if the clustering satisfies the niceness conditions.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm}
\subsection{Based on the knowledge of $\max r(B_i)$}
Given a set $\mathcal{X}$, we assume that our algorithm knows the value of the maximum radius. Iteratively, we find the densest ball of twice the maximum radius. Delete some points and repeat till we find $k$ such balls. The algorithm is described in detail below.

\begin{algorithm}
\begin{alg}
%\item[] for $i=1$ to $l$
%\begin{itemize}
%	\item[] Let $L_i$ denote the set of balls found in $i^{th}$ iteration. Initially $L_i=\phi$
%	\item[] set $d=d_i$
	\item[] Set $r = \max_i \thinspace r(B_i)$
	\item[] for $i=1$ to $k$
	\begin{itemize}
		\item[] Let $B_i'$ be the densest ball of radius $2r$ such that $c(B_i') \in \mathcal{X}$.
		\item[] Set $\mathcal{X}=\mathcal{X}\setminus B_{4r}(c(B_i'))$. 
	\end{itemize}
	%\item[] If the clustering $L_i$ is nice
	%\begin{itemize}
%	\item[] $L := L \cup L_i$.
	%\end{itemize}
%\end{itemize}
Output the clustering $C'$ induced by $B' = \{B_1',\ldots,B_k'\}$.
\label{alg:NotKnown}
\end{alg}
\caption{Alg. for known $\max{r(B_i)}$}
\end{algorithm}

\begin{theorem}
Given a $(2,8)$-nice set $\mathcal{X}$. Let $B = \{B_1,\ldots,B_k\}$ be the set of any balls that satisfy the separation and sparseness condition. Given $r = \max r(B_i)$, then Algorithm \ref{alg:NotKnown} outputs a set $B' = \{B_1',\ldots,B_k'\}$ such that 
\begin{itemize}
	\item The sets $B$ and $B'$ are unique under intersection. That is, for all $i\neq j$, $B_i \cap B_j' = \phi$ and for all $i$, $B_i \cap B_i' \neq \phi$
	\item The clustering $C'$ induced by $B'$ is such that $C'|_B = \{B_1,\ldots,B_k\}$.	
\end{itemize}

\end{theorem}
\begin{proof}
Assume that $B_1'$ is such that for all $i$, $B_1' \cap B_i = \phi$. Observe that $B_1' \cap \{X \backslash \cup_{i\in[k]} B_i\} = B_1'$. Observe that $B_1'$ is the densest ball of radius $2r$ with $c(B_1') \in \mathcal{X}$. Hence, $|B_1'| \ge |B_i|$ for all $i$. Hence, $|B_1'| \ge \min |B_i|$. Thus, $|B_1' \cap \{X \backslash \cup_{i\in[k]} B_i\}| \ge \min |B_i|$ which contradicts the fact that $B$ satisfies the sparseness condition. 

Without loss of generality, let $B_1' \cap B_1 \neq \phi$. Separation $\nu = 8$ implies that for all $i\neq 1$, $B_1' \cap B_i = \phi$ . Since, $B_1' \cap B_1 \neq \phi$, $|c(B_1)-c(B_1')| \le 3r$ and for all $i\neq 1$, $|c(B_1')-c(B_i)| > 5r$. Hence, setting $\mathcal{X} = \mathcal{X}\setminus B_{4r}(c(B_1'))$ ensures that $\mathcal{X} \cap B_1 = \phi$ and for all $i\neq 1$, $\mathcal{X} \cap B_i = B_i$. Similarly, we get that $B_2' \cap B_2 \neq \phi$ and for all $i \neq 2$, $B_2' \cap B_i = \phi$.

Let $x \in B_i$. Now, $|x-c(B_i')| \le |x-c(B_i)| + |c(B_i')-c(B_i)| \le r+3r = 4r$. For $j \neq i$, $|x-c(B_j')| \ge |c(B_j')-c(B_i)| - |x-c(B_i)| > 5r - r = 4r$. Hence, we see that the clustering $C'$ assigns $x$ to the $i^{th}$ cluster. Hence, $C'|_B = \{B_1,\ldots,B_k\}$
\end{proof}

\begin{theorem}
Algorithm \ref{alg:NotKnown} runs in time $O($poly$(|\mathcal{X}|,k))$.
\end{theorem}
\begin{proof}
During each iteration, we find the densest ball of radius $2r$. For each point in $\mathcal{X}$, we count the number of points whose distance is $\le 2r$. This can take $O(n)$ in the worst case. Hence, finding the densest ball takes $O(n^2)$ time. Similarly, deleting points from $\mathcal{X}$ takes $O(n^2)$ time. Hence, the for-loop takes $O(n^2k)$ time.
\end{proof}

\subsection{Based on the knowledge of $\min |B_i|$}
%All algoritthms are polynomial in n and k
In this case, we assume that the algorithm doesn't know the value of $r = \max r(B_i)$. We assume that the algorithm knows the number of points in the smallest cluster $\min |B_i| = t$ (say). This algorithm works in two phases.

\begin{algorithm}
\begin{alg}
\item[] \textbf{Phase 1:}
\begin{itemize}
\item[] Let $L$ denote the balls found so far. Initialize $L = \phi$.
\item[] Let $B_S$ be smallest ball such that $c(B_S) \in \mathcal{X}$, $|B_S| \ge t$  and $|B_S \cap L | = \phi$.
\item[] while $B_S$ exists
\begin{itemize}
\item[] $L = L \cup B_S$. 
\item[] $\mathcal{X} = \mathcal{X}\setminus B_S$.
\end{itemize}
\item[] $L_C = c(L)$. Output the list of centers $L_C$ to the second phase.
\end{itemize}

\item[] \textbf{Phase 2:}
\begin{itemize}
\item[] Let $L_C = c_1,\ldots,c_p$.
\item[] Let $L$ denote the list of nice clusterings found so far. Initialize $L := \phi$
\item[] Let $T$ denote the hierarchical clustering tree. Initialize $T$ to be the tree with nodes $c_1,\ldots,c_k$ without any edges.
\item[] Let $D$ denote the current clustering. Initialize $D = (c_1,\ldots.c_k)$.
\item[] for $i=k+1$ to $p$
\begin{itemize}
\item[] Merge the two closest clusters in $D\cup c_i$ to get a new $k$-clustering. 
\item[] Merge the two clusters in the tree $T$. Update $D$. 
\item[] If $D$ is a nice clustering
\begin{itemize}
\item[] $L = L \cup D$.
\end{itemize}
\end{itemize}

\item[] // We built a forest with $k$ trees corresponding to the $k$ different clusters we found. 
\item[] Use single linkage to merge the $k$-clusters found so far. Update $T$.
\item[] Output $L$ and $T$.
\end{itemize}
\label{alg:Known}
\end{alg}
\caption{Alg. for known $\min{B_i}$}
\end{algorithm}

\begin{theorem}
Given a $(2,8)$-nice set $\mathcal{X}$. Let $B = \{B_1,\ldots,B_k\}$ be the set of any balls that satisfy the separation and sparseness condition. Given $t = \min |B_i|$, Algorithm \ref{alg:Known} outputs 
\begin{itemize}
\item A list $L$ of possible clusterings such that $|L| \in O(|\mathcal{X}|)$. $L$ contains a clustering $C$ such that $C|_B = \{B_1,\ldots,B_k\}$.
\item A hierarchical clustering tree $T$. A pruning of this tree corresponds to a clustering $C$ such that $C|_B = \{B_1,\ldots,B_k\}$.
\end{itemize}
\end{theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
