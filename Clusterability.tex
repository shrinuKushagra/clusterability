\documentclass[11pt]{article}


%%%%% Packages %%%%%
\usepackage[paper]{nickstyle}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb, amsmath}



%%%%% Macros %%%%%
\setlength{\parindent}{24pt}
\renewcommand{\bar}[1]{\overline{#1}}

\newtheorem{fact}[theorem]{Fact}
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{problem}[theorem]{Problem}
\newcommand{\ConjName}[1]{\label{con:#1}}
\newcommand{\Conj}[1]{Conjecture~\ref{con:#1}}
\newcommand{\ProblemName}[1]{\label{prob:#1}}
\newcommand{\Problem}[1]{Problem~\ref{prob:#1}}
\renewcommand{\dot}{\bullet}
\newcommand{\Tr}{\operatorname{tr}}
\newcommand{\eps}{\epsilon}

\newcommand{\lmax}{\lambda_\mathrm{max}}
\newcommand{\lmin}{\lambda_\mathrm{min}}
\newcommand{\ufinal}{u_\mathrm{final}}
\newcommand{\lfinal}{l_\mathrm{final}}
\newcommand{\umax}{u_\mathrm{max}}

\newcommand{\Symraw}{\mathbb{S}}
\newcommand{\Sym}[1][]{\Symraw^{\ifthenelse{\equal{#1}{}}{m}{#1}}}
\newcommand{\Psd}[1][]{\Symraw_+^{\ifthenelse{\equal{#1}{}}{m}{#1}}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\iprod}[2]{\langle #1, #2 \rangle}
\newcommand{\paren}[2][]{#1({#2}#1)}
\newcommand{\qform}[2]{\transp{#2}#1#2}
\newcommand{\transp}[1]{#1^T}



% Simple (outer) environment for algorithms
\newenvironment{outer_alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
        \setlength{\leftmargin}{5pt}
    }
}
{
    \end{list}
}

% Simple environments for algorithms
\newenvironment{alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
    }
}
{
    \end{list}
}




%%%%% Title %%%%%
\title{\LARGE Noise robust clustering}
\author{}


%%%%% Document Body %%%%%
\begin{document}
\maketitle

\section{Introduction}
\begin{itemize}
\item Nika and Shai ICML'14
\end{itemize}

\section{Problem Statement}

\subsection{Notation}
For any set $B\subset X$, we denote $c(B)$ as the center of $B$ which is defined as the average of points in $B$. Radius of the set $B$ is defined as $r(B)=\max_{x\in B} |x-c(B)|$. Define the induced clustering.

\begin{definition}[Niceness assumption]
% Adding C_{k+1} as garbage collector?
Given a set $\mathcal{X}$, we say that $\mathcal{X}$ is $(\lambda,\nu)$-nice if there exists a clustering of $\mathcal{X}$, $P=\{P_1,\ldots,P_k\}$ which satisfies the following conditions. There exists balls $B=\{B_1,\ldots,B_k\}\subset \mathcal{X}$ such that for every $i\in[k]$, there exists $j_i\in[k]$ such that $B_i\subset P_{j_i}$ and
\begin{itemize}
\item{\bf{Separation}:} For all $i,j\in[k], |c(B_i)-c(B_j)|\geq \nu\cdot\max\{r(B_i),r(B_j)\}$
\item{\bf{Sparse Noise}}: For any ball $B$ such that $c(B)\in \mathcal{X}$, if $r(B)\leq \lambda \cdot \max \thinspace r(B_i)$, then $|B\cap \{X \backslash \cup_{i\in[k]} B_i\}| < \min_i |B_i|$.
\end{itemize}
\end{definition}

\noindent {\bf Goal}: Given the set $\mathcal{X}$ and the value $k$, our goal is to design an algorithm that does $k$-clustering $C=\{C_1,\ldots,C_k\}$ on set $\mathcal{X}$ such that  for any $(\lambda,\nu)$-nice clustering $P$ with sets $B = \{B_1,\ldots,B_k\}$ the induced clustering of $C$ on $B$, i.e., $C|_B = \{B_1,\ldots,B_k\}$. 

\begin{lemma}
\label{lemma:chknice}
Given a $(\lambda,\nu)$-nice set $\mathcal{X}$. Given a $k$-clustering $C = \{C_1,\ldots,C_k\}$ of $\mathcal{X}$ and corresponding balls $B = \{B_1,\ldots,B_k\}$, i.e. $B_i \subseteq C_i$, we can check if $C$ is nice in time poynomial in $|\mathcal{X}|$ and $k$.
\end{lemma}
\begin{proof}
We can check if the balls $B_i$ satisfy the separation condition in time $O(k^2)$. To check if they satisfy the sparse noise condition, it is easy to see that it takes time $O(n^2)$ where $n = |\mathcal{X}|$. Hence, given the set of balls and the clustering induced by the balls $(C,B)$, we can test in $O(n^2+k^2)$ time if the clustering satisfies the niceness conditions.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm}

\subsection{Iterate over radius}

Given the set $\mathcal{X}$,
we consider all the $l$ possible different distances between elements of the set $\mathcal{X}$. We know that there are $O(n^2)$ such distances. We sort them as $d_1<\ldots<d_l$. The algorithm works iteratively as following:


\begin{algorithm}
\begin{alg}
\item[] for $i=1$ to $l$
\begin{itemize}
	\item[] Let $L_i$ denote the set of balls found in $i^{th}$ iteration. Initially $L_i=\phi$
	\item[] set $d=d_i$
	\item[] for $j=1$ to $k$
	\begin{itemize}
		\item[] Let $B$ be the densest ball of radius $2d$ such that $c(B) \in \mathcal{X}$.
		\item[] Add $B$ to $L_i$. Set $\mathcal{X}=\mathcal{X}\setminus B_{4d}(c(B))$. 
	\end{itemize}
	\item[] If the clustering $L_i$ is nice
	\begin{itemize}
		\item[] $L := L \cup L_i$.
	\end{itemize}
\end{itemize}
Output the list of clusters $L$.
\label{alg:NotKnown}
\end{alg}
\caption{Alg. for unknown $\min{B_i}$}
\end{algorithm}

\begin{theorem}
Given a $(2,8)$-nice set $\mathcal{X}$, Algorithm \ref{alg:NotKnown} outputs a list $L$ of possible nice clusterings such that $|L| \in O(|\mathcal{X}|^2)$ and $L$ contains a clustering $C$ such that $C|_B = \{B_1,\ldots,B_k\}$.
\end{theorem}
\begin{proof}
\end{proof}

\begin{theorem}
Algorithm \ref{alg:NotKnown} runs in time $O($poly$(|\mathcal{X}|,k))$.
\end{theorem}
\begin{proof}
The outermost for-loop considers $l$ possible distance where $l \in O(n^2)$. The inner for-loop runs $k$ times. During each iteration, we find the densest ball of some given radius ($2d$). For each point in $\mathcal{X}$, we count the number of points whose distance is $\le 2d$. This can take $O(n)$ in the worst case. Hence, finding the densest ball takes $O(n^2)$ time. Similarly, deleting points from $\mathcal{X}$ takes $O(n^2)$ time. Hence, the inner for loop takes $O(n^4k)$ time.

Using Lemma \ref{lemma:chknice}, we see that checking if a clustering is nice takes $O(n^2+k^2)$ time. Hence, for all $l$ values, this takes $O(n^4+n^2k^2)$ time. Hence, the algorithm takes $O(n^4k+n^2k^2)$ time which is polynomial in $n$ and $k$.
\end{proof}

\subsection{Based on the knowledge of $\min |B_i|$}
%All algoritthms are polynomial in n and k
The algorithm gets as input $\mathcal{X}$ and the number of points in the smallest cluster $\min |B_i| = t$ (say). This algorithm works in two phases.

\begin{algorithm}
\begin{alg}
\item[] \textbf{Phase 1:}
\begin{itemize}
\item[] Let $L$ denote the balls found so far. Initialize $L = \phi$.
\item[] Let $B_S$ be smallest ball such that $c(B_S) \in \mathcal{X}$, $|B_S| \ge t$  and $|B_S \cap L | = \phi$.
\item[] while $B_S$ exists
\begin{itemize}
\item[] $L = L \cup B_S$. 
\item[] $\mathcal{X} = \mathcal{X}\setminus B_S$.
\end{itemize}
\item[] $L_C = c(L)$. Output the list of centers $L_C$ to the second phase.
\end{itemize}

\item[] \textbf{Phase 2:}
\begin{itemize}
\item[] Let $L_C = c_1,\ldots,c_p$.
\item[] Let $L$ denote the list of nice clusterings found so far. Initialize $L := \phi$
\item[] Let $T$ denote the hierarchical clustering tree. Initialize $T$ to be the tree with nodes $c_1,\ldots,c_k$ without any edges.
\item[] Let $D$ denote the current clustering. Initialize $D = (c_1,\ldots.c_k)$.
\item[] for $i=k+1$ to $p$
\begin{itemize}
\item[] Merge the two closest clusters in $D\cup c_i$ to get a new $k$-clustering. 
\item[] Merge the two clusters in the tree $T$. Update $D$. 
\item[] If $D$ is a nice clustering
\begin{itemize}
\item[] $L = L \cup D$.
\end{itemize}
\end{itemize}

\item[] // We built a forest with $k$ trees corresponding to the $k$ different clusters we found. 
\item[] Use single linkage to merge the $k$-clusters found so far. Update $T$.
\item[] Output $L$ and $T$.
\end{itemize}
\label{alg:Known}
\end{alg}
\caption{Alg. for known $\min{B_i}$}
\end{algorithm}

\begin{theorem}
Given a $(2,8)$-nice set $\mathcal{X}$, Algorithm \ref{alg:Known} outputs 
\begin{itemize}
\item A list $L$ of possible clusterings such that $|L| \in O(|\mathcal{X}|)$. $L$ contains a clustering $C$ such that $C|_B = \{B_1,\ldots,B_k\}$.
\item A hierarchical clustering tree $T$. A pruning of this tree corresponds to a clustering $C$ such that $C|_B = \{B_1,\ldots,B_k\}$.
\end{itemize}
\end{theorem}

\noindent Note that Alg \ref{alg:NotKnown}, outputs a list of possible clusterings with size $O(|\mathcal{X}|^2)$. However, with the knowledge of another parameter ($\min|B_i|$), Alg \ref{alg:Known} improves the size of this list to $O(|\mathcal{X}|)$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\end{document}
